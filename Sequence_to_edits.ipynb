{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "\n",
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from seqeval.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728213c",
   "metadata": {},
   "source": [
    "##### instal packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d27235ca",
   "metadata": {},
   "source": [
    "!conda install pytorch=1.9 torchvision torchaudio -c pytorch\n",
    "!conda install -y torchtext==0.10 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cba2cf",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #1,2,4,8,16,32,64,128,256,512,1028\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4c24",
   "metadata": {},
   "source": [
    "#### seed initializing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81441ae5",
   "metadata": {},
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "torch.backends.cudnn.determininistic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space\n",
    "3. Build a vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "# removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"0\")\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"0\")\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(path='data',train='ptrain.jsonl',test='val.jsonl',\n",
    "                                              format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')\n",
    "edits.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "sort_by_len = lambda x:len(x)\n",
    "\n",
    "train_data_iterator = BucketIterator(train_data,train=True,\n",
    "                                            batch_size=BATCH_SIZE, device= device)#,sort_within_batch=False)\n",
    "\n",
    "test_data_iterator =BucketIterator(test_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e5f1",
   "metadata": {},
   "source": [
    "#### data processing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97f75e0",
   "metadata": {
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7feffac10690>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['alistair', 'darling', 'is', 'expected', 'to', 'announce', 'details', 'of', 'tax', 'cuts', 'and', 'plans', 'to', 'increases', 'public', 'spending'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$append_.']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518a8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = vars(test_data.examples[0])    # to print tokens of any row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7074d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 =len(test_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddfe6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afbb1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 48])\n",
      "torch.Size([36, 47])\n"
     ]
    }
   ],
   "source": [
    "for i in test_data_iterator:\n",
    "    print(i.edits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d80e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef8b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['$$$'] = 0\n",
      "tokens.vocab.itos[0] = <unk>\n",
      "edits.vocab.stoi['$keep'] = 2\n",
      "edits.vocab.itos[2] = $transform_agreement_plural\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['$$$'] = {tokens.vocab.stoi['$$$']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[0]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['$keep']}\")\n",
    "print(f\"edits.vocab.itos[2] = {edits.vocab.itos[13]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab) = 1482\n",
      "len(edits.vocab) = 22\n"
     ]
    }
   ],
   "source": [
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3243d5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab.freqs.keys()) = 1480\n",
      "len(edits.vocab.freqs.keys()) = 20 \n",
      "\n",
      "edits.vocab.freqs = Counter({'$keep': 2892, '$delete': 103, '$replace_,': 25, '$replace_.': 16, '$transform_agreement_singular': 15, '$append_.': 13, '$replace_the': 13, '$transform_verb_vbg_vb': 10, '$append_the': 9, '$transform_agreement_plural': 9, '$replace_to': 9, '$replace_of': 9, '$transform_verb_vbz_vb': 8, '$append_and': 7, '$append_a': 6, '$append_of': 6, '$append_to': 6, '$append_,': 6, '$replace_in': 6, '$transform_verb_vbn_vb': 3})\n"
     ]
    }
   ],
   "source": [
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {edits.vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae29120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.vectors.shape = torch.Size([1482, 100])\n",
      "edits.vocab.vectors.shape = torch.Size([22, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens.vocab.vectors.shape}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits.vocab.vectors.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7484bbd",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9881b4ee",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        self.position_embedding_layer = nn.Embedding(max_lenght,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\") \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        \n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        position = torch.arange(0,num_of_tokens).unsqueeze(0)         # [1,num_of_tokens]\n",
    "        position = position.repeat(batch_size,1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.token_embedding_layer(tokens)\n",
    "        # position_embeddings = self.position_embedding_layer(position)\n",
    "        # token_embeddings = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        # elementwise sum of tokens and position \n",
    "        combination = self.dropout(token_embeddings)#+position_embeddings)\n",
    "        # combination = [batch_size, num_of_tokens,embedding dimension]\n",
    "        #print(f\"batch_size, num_of_tokens,embedding dimension = {combination.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conved = combination.reshape(batch_size,-1).unsqueeze(1)\n",
    "        #conv_input = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        #print(f\"conv lay input = {conved.shape}\")\n",
    "        \n",
    "        # multiple convolution and activation layers\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv_layer(self.dropout(conved))\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "            \n",
    "            #pass through relu activation function\n",
    "            conved = F.relu(conved)*self.scale\n",
    "\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        \n",
    "        #print(f\"conv lay output = {conved.shape}\")\n",
    "        \n",
    "        conved = conved.squeeze(1).reshape(batch_size,num_of_tokens,-1)\n",
    "        #conved = [batch_size, num_of_tokens, embedding_dimension]\n",
    "        \n",
    "        \n",
    "        output = self.final_layer(conved+combination)*self.scale\n",
    "        #output = [batch size, num_of_tokens, target dimension]\n",
    "        #print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "691c256f",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\",device=self.device) \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(num_of_filters*embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "     def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.dropout(self.token_embedding_layer(tokens))\n",
    "        # token_embeddings = [batch_size, num_of_tokens, embedding dimension]\n",
    "        # print(f\"batch_size, num_of_tokens,embedding_dimension = {token_embeddings.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conv_input = token_embeddings.reshape(batch_size,-1).unsqueeze(1)\n",
    "        # conv_input = [batch size, 1, embedding_dimension*num_of_tokens]\n",
    "        # print(f\"conv lay input = {conv_input.shape}\")\n",
    "        \n",
    "        conved = [conv_layer(conv_input).squeeze(1) for conv_layer in self.conv_layers]\n",
    "        \n",
    "        cat = self.dropout(F.relu(torch.cat(conved, dim = 1)))\n",
    "        # [batch size, embedding dimension*num_of_tokens*num_of_filter]\n",
    "\n",
    "        conved = cat.reshape(batch_size,num_of_tokens,-1)\n",
    "        # conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "        # print(f\"conved before output = {conved.shape}\")\n",
    "        \n",
    "        output = self.final_layer(conved)*self.scale\n",
    "        # output = [batch size, num_of_tokens, target dimension]\n",
    "        # print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad412d2",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07bcb53",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 75\n",
    "clip = 0.1\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "FILTER_SIZES = [1,2,3]\n",
    "N_FILTERS = len(FILTER_SIZES)\n",
    "DROPOUT = 0.75\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074921ca",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c38ca13d",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq_to_edits(INPUT_DIM,OUTPUT_DIM,OUTPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,DROPOUT,PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fa205d5",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 160,431 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862f3c77",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.6149,  0.9273,  0.5583,  ..., -0.0922, -0.0798, -1.2843],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.7497,  0.3603,  0.9107,  ...,  0.1232,  0.2503, -0.1138],\n",
       "        [ 0.7090,  0.6810, -1.2128,  ...,  0.6917,  1.4080, -1.0227],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-tained embeddings \n",
    "\n",
    "pretrained_embeddings = tokens.vocab.vectors\n",
    "\n",
    "model.token_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c21b7ae",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "\n",
    "model.token_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.token_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d081c6",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f7907ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq_to_edits(\n",
       "  (token_embedding_layer): Embedding(1482, 100)\n",
       "  (position_embedding_layer): Embedding(100, 100)\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(1, 1, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(1, 1, kernel_size=(2,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=100, out_features=22, bias=True)\n",
       "  (dropout): Dropout(p=0.75, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0798e",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f71b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,data_iterator,optimizer,criterion,clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "        \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        \n",
    "        # Make the gradient vector to zero so \n",
    "        #So not to added with the new gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        # get output from the model\n",
    "        outputs = model(tokens_list)\n",
    "        # outputs = [batch size, num_of_tokens, target dimension]\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "        # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "        trg = batch.edits.contiguous().view(-1)\n",
    "        # trg = [batch size*num_of_tokens]\n",
    "        # print(f\"out dim = {outputs_1.shape}\")\n",
    "        \n",
    "        loss = criterion(outputs, trg)\n",
    "\n",
    "        # Backward and optimize\n",
    "        \n",
    "        # to calculate gradient\n",
    "        loss.backward()\n",
    "        #to avoid exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # to make the updates in the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        predicted = torch.argmax(outputs, dim=-1)\n",
    "        # print(f\"pre = {predicted.shape}\")\n",
    "        # print(f\"edit = {trg.shape}\")\n",
    "        \n",
    "        acc += accuracy_score(predicted, trg).item()\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "921e2892",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            # converting to cpu or gpu variable\n",
    "            tokens_list = batch.tokens.to(device)\n",
    "            edits_list = batch.edits.to(device)\n",
    "            \n",
    "            outputs = model(tokens_list)\n",
    "            # outputs = [batch size, num_of_tokens, embedding dimension]\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "            # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "            trg = batch.edits.contiguous().view(-1)\n",
    "            # trg = [batch size*num_of_tokens]\n",
    "            loss = criterion(outputs, trg)\n",
    "            epoch_loss += loss.item() \n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "            # print(f\"pre = {predicted.shape}\")\n",
    "            # print(f\"edit = {edits_list.shape}\")\n",
    "\n",
    "            acc += accuracy_score(predicted, trg).item()\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddc3b",
   "metadata": {},
   "source": [
    "#### actual train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38206959",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2223181f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Train Loss: 2.996, Train Accuracy: 18.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirthika-13378/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/75], Train Loss: 2.483, Train Accuracy: 61.814\n",
      "Epoch [3/75], Train Loss: 2.014, Train Accuracy: 75.268\n",
      "Epoch [4/75], Train Loss: 1.590, Train Accuracy: 80.603\n",
      "Epoch [5/75], Train Loss: 1.230, Train Accuracy: 82.667\n",
      "Epoch [6/75], Train Loss: 0.957, Train Accuracy: 84.075\n",
      "Epoch [7/75], Train Loss: 0.780, Train Accuracy: 83.719\n",
      "Epoch [8/75], Train Loss: 0.671, Train Accuracy: 84.605\n",
      "Epoch [9/75], Train Loss: 0.628, Train Accuracy: 85.096\n",
      "Epoch [10/75], Train Loss: 0.589, Train Accuracy: 86.061\n",
      "Epoch [11/75], Train Loss: 0.527, Train Accuracy: 89.334\n",
      "Epoch [12/75], Train Loss: 0.489, Train Accuracy: 90.211\n",
      "Epoch [13/75], Train Loss: 0.438, Train Accuracy: 90.927\n",
      "Epoch [14/75], Train Loss: 0.407, Train Accuracy: 91.478\n",
      "Epoch [15/75], Train Loss: 0.381, Train Accuracy: 91.836\n",
      "Epoch [16/75], Train Loss: 0.341, Train Accuracy: 92.795\n",
      "Epoch [17/75], Train Loss: 0.343, Train Accuracy: 92.833\n",
      "Epoch [18/75], Train Loss: 0.295, Train Accuracy: 94.028\n",
      "Epoch [19/75], Train Loss: 0.274, Train Accuracy: 94.248\n",
      "Epoch [20/75], Train Loss: 0.262, Train Accuracy: 94.669\n",
      "Epoch [21/75], Train Loss: 0.254, Train Accuracy: 94.780\n",
      "Epoch [22/75], Train Loss: 0.240, Train Accuracy: 94.831\n",
      "Epoch [23/75], Train Loss: 0.235, Train Accuracy: 95.022\n",
      "Epoch [24/75], Train Loss: 0.230, Train Accuracy: 94.951\n",
      "Epoch [25/75], Train Loss: 0.213, Train Accuracy: 95.284\n",
      "Epoch [26/75], Train Loss: 0.213, Train Accuracy: 95.284\n",
      "Epoch [27/75], Train Loss: 0.200, Train Accuracy: 95.506\n",
      "Epoch [28/75], Train Loss: 0.195, Train Accuracy: 95.486\n",
      "Epoch [29/75], Train Loss: 0.193, Train Accuracy: 95.440\n",
      "Epoch [30/75], Train Loss: 0.186, Train Accuracy: 95.471\n",
      "Epoch [31/75], Train Loss: 0.178, Train Accuracy: 95.699\n",
      "Epoch [32/75], Train Loss: 0.179, Train Accuracy: 95.523\n",
      "Epoch [33/75], Train Loss: 0.177, Train Accuracy: 95.636\n",
      "Epoch [34/75], Train Loss: 0.174, Train Accuracy: 95.941\n",
      "Epoch [35/75], Train Loss: 0.173, Train Accuracy: 95.713\n",
      "Epoch [36/75], Train Loss: 0.166, Train Accuracy: 95.891\n",
      "Epoch [37/75], Train Loss: 0.175, Train Accuracy: 95.875\n",
      "Epoch [38/75], Train Loss: 0.157, Train Accuracy: 96.181\n",
      "Epoch [39/75], Train Loss: 0.159, Train Accuracy: 96.131\n",
      "Epoch [40/75], Train Loss: 0.160, Train Accuracy: 95.989\n",
      "Epoch [41/75], Train Loss: 0.155, Train Accuracy: 96.167\n",
      "Epoch [42/75], Train Loss: 0.173, Train Accuracy: 95.770\n",
      "Epoch [43/75], Train Loss: 0.155, Train Accuracy: 96.213\n",
      "Epoch [44/75], Train Loss: 0.153, Train Accuracy: 96.055\n",
      "Epoch [45/75], Train Loss: 0.154, Train Accuracy: 96.150\n",
      "Epoch [46/75], Train Loss: 0.159, Train Accuracy: 96.009\n",
      "Epoch [47/75], Train Loss: 0.149, Train Accuracy: 96.117\n",
      "Epoch [48/75], Train Loss: 0.142, Train Accuracy: 96.503\n",
      "Epoch [49/75], Train Loss: 0.152, Train Accuracy: 96.107\n",
      "Epoch [50/75], Train Loss: 0.139, Train Accuracy: 96.406\n",
      "Epoch [51/75], Train Loss: 0.138, Train Accuracy: 96.521\n",
      "Epoch [52/75], Train Loss: 0.138, Train Accuracy: 96.504\n",
      "Epoch [53/75], Train Loss: 0.140, Train Accuracy: 96.440\n",
      "Epoch [54/75], Train Loss: 0.137, Train Accuracy: 96.712\n",
      "Epoch [55/75], Train Loss: 0.140, Train Accuracy: 96.469\n",
      "Epoch [56/75], Train Loss: 0.133, Train Accuracy: 96.618\n",
      "Epoch [57/75], Train Loss: 0.133, Train Accuracy: 96.407\n",
      "Epoch [58/75], Train Loss: 0.137, Train Accuracy: 96.372\n",
      "Epoch [59/75], Train Loss: 0.133, Train Accuracy: 96.360\n",
      "Epoch [60/75], Train Loss: 0.142, Train Accuracy: 96.248\n",
      "Epoch [61/75], Train Loss: 0.134, Train Accuracy: 96.585\n",
      "Epoch [62/75], Train Loss: 0.133, Train Accuracy: 96.394\n",
      "Epoch [63/75], Train Loss: 0.134, Train Accuracy: 96.585\n",
      "Epoch [64/75], Train Loss: 0.130, Train Accuracy: 96.547\n",
      "Epoch [65/75], Train Loss: 0.126, Train Accuracy: 96.571\n",
      "Epoch [66/75], Train Loss: 0.134, Train Accuracy: 96.334\n",
      "Epoch [67/75], Train Loss: 0.131, Train Accuracy: 96.728\n",
      "Epoch [68/75], Train Loss: 0.127, Train Accuracy: 96.530\n",
      "Epoch [69/75], Train Loss: 0.130, Train Accuracy: 96.472\n",
      "Epoch [70/75], Train Loss: 0.130, Train Accuracy: 96.521\n",
      "Epoch [71/75], Train Loss: 0.124, Train Accuracy: 96.713\n",
      "Epoch [72/75], Train Loss: 0.120, Train Accuracy: 96.634\n",
      "Epoch [73/75], Train Loss: 0.125, Train Accuracy: 96.662\n",
      "Epoch [74/75], Train Loss: 0.134, Train Accuracy: 96.517\n",
      "Epoch [75/75], Train Loss: 0.124, Train Accuracy: 96.554\n"
     ]
    }
   ],
   "source": [
    "# start = time.perf_counter()\n",
    "\n",
    "# try:\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss,train_acc = train_model(model,train_data_iterator,optimizer,criterion,clip)\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}')\n",
    "    loss.append(train_loss)\n",
    "    accuracy.append(train_acc)\n",
    "    \n",
    "# finally:\n",
    "#     end = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "203bb574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "\n",
    "axs[0].plot(range(len(loss)),loss)\n",
    "axs[1].plot(range(len(accuracy)),accuracy)\n",
    "\n",
    "plt.setp(axs[1], xlabel='num of epoch')\n",
    "\n",
    "plt.setp(axs[0], ylabel='loss')\n",
    "plt.setp(axs[1], ylabel='acc')\n",
    "\n",
    "plt.savefig('my_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b62f2fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('my_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b991dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.247 Test accuracy 96.267\n"
     ]
    }
   ],
   "source": [
    "test_loss,accuracy = evaluate_model(model, test_data_iterator, criterion)\n",
    "print(f\"Test Loss {test_loss:.3f} Test accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[25, 13, 14, 29, 13, 13, 13, 25, 13, 13, 25, 21, 13, 13, 29, 13,  2,  2,\n",
    "        13, 13, 13, 25]\n",
    "b=[2, 2, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2517bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d6c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101710ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c5387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n",
    "\n",
    "for batch in train_data_iterator:\n",
    "    x = emb_1(batch.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.permute(0,2,1).reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 1)(x_input)\n",
    "b = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 2)(a)\n",
    "c = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 6)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.squeeze(1).reshape(2,5,-1)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f = {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = nn.Conv1d(in_channels =1 ,out_channels= 4,kernel_size = 3)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"g = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1b60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8328aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0184d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c2d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec9b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26565c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9422f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafe25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785caf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act = torch.Tensor([2,0,1])\n",
    "y_pred = torch.argmax(torch.Tensor([[0.1,3,5],[3,2,1],[0,1,0]]),dim=1)\n",
    "y_pred_1 = torch.argmax(torch.Tensor([[1,5,2],[3,2,1],[0,1,0]]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69befbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_1,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb1fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea2fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac44f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221603e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f25d69",
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 1,padding=\"same\")(x_input).squeeze(1)\n",
    "c1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 3,padding=\"same\")(x_input).squeeze(1)\n",
    "d1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 5,padding=\"same\")(x_input).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.cat((b1,c1,d1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b72f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5da08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb6ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b14c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.arange(0,10,1)\n",
    "y1 = np.random.randn(10)\n",
    "y3 = np.random.randn(10)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, sharex='col')\n",
    "ax[0].plot(x,y1)\n",
    "ax[1].plot(x,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6332fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.sqrt(torch.FloatTensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42458479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eced070",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conv 1D layers \n",
    "conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                            out_channels= 1,\n",
    "                                            kernel_size = filter_size,\n",
    "                                            padding = \"same\") \n",
    "                                  for filter_size in [1,3,5]])\n",
    "\n",
    "final_layer = nn.Linear(5*3, 3)\n",
    "\n",
    "# Dropout layers\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "# token_embeddings = [2, 1, 4*5]\n",
    "\n",
    "conved = [F.relu(conv_layer(x_input)).squeeze(1) for conv_layer in conv_layers]\n",
    "print(len(conved))\n",
    "concat = dropout(torch.cat(conved, dim = 1))\n",
    "# [batch size, embedding dimension*num_of_tokens*num_of_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99404586",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conved = cat.reshape(2,4,-1)\n",
    "# conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "conved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfff0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = final_layer(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af44568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17b65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65710e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a7c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199af3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdb4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48c1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efba383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn3]",
   "language": "python",
   "name": "conda-env-nn3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "265px",
    "width": "201px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1d36ce9c6112765ad20cbf471b33373de7b9da6b2d9a2b18d6540897511eb08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
