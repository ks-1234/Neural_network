{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "\n",
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from seqeval.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06f1da",
   "metadata": {},
   "source": [
    "##### instal packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d488e8f",
   "metadata": {},
   "source": [
    "!conda install pytorch=1.9 torchvision torchaudio -c pytorch\n",
    "!conda install -y torchtext==0.10 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cba2cf",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 #1,2,4,8,16,32,64,128,256,512,1028\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4c24",
   "metadata": {},
   "source": [
    "#### seed initializing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06279596",
   "metadata": {},
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "torch.backends.cudnn.determininistic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space\n",
    "3. Build a vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "# removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"0\")\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"0\")\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(path='data',train='ptrain.jsonl',test='val.jsonl',\n",
    "                                              format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1,vectors='fasttext.simple.300d')\n",
    "edits.build_vocab(train_data,min_freq=1,vectors='fasttext.simple.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bd556948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charngram.100d',\n",
       " 'fasttext.en.300d',\n",
       " 'fasttext.simple.300d',\n",
       " 'glove.42B.300d',\n",
       " 'glove.840B.300d',\n",
       " 'glove.twitter.27B.25d',\n",
       " 'glove.twitter.27B.50d',\n",
       " 'glove.twitter.27B.100d',\n",
       " 'glove.twitter.27B.200d',\n",
       " 'glove.6B.50d',\n",
       " 'glove.6B.100d',\n",
       " 'glove.6B.200d',\n",
       " 'glove.6B.300d']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', \n",
    " 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', \n",
    " 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "sort_by_len = lambda x:len(x)\n",
    "\n",
    "train_data_iterator = BucketIterator(train_data,train=True,\n",
    "                                            batch_size=BATCH_SIZE, device= device)#,sort_within_batch=False)\n",
    "\n",
    "test_data_iterator =BucketIterator(test_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e5f1",
   "metadata": {},
   "source": [
    "#### data processing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e97f75e0",
   "metadata": {
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7fcff9047dd0>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['alistair', 'darling', 'is', 'expected', 'to', 'announce', 'details', 'of', 'tax', 'cuts', 'and', 'plans', 'to', 'increases', 'public', 'spending'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$append_.']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "518a8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = vars(test_data.examples[0])    # to print tokens of any row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7074d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 =len(test_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ca2a5ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9d3e8e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 48])\n"
     ]
    }
   ],
   "source": [
    "for i in test_data_iterator:\n",
    "    print(i.edits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3ef8b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['$$$'] = 0\n",
      "tokens.vocab.itos[0] = <unk>\n",
      "edits.vocab.stoi['$keep'] = 2\n",
      "edits.vocab.itos[2] = $transform_agreement_plural\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['$$$'] = {tokens.vocab.stoi['$$$']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[0]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['$keep']}\")\n",
    "print(f\"edits.vocab.itos[2] = {edits.vocab.itos[13]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "547693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab) = 1482\n",
      "len(edits.vocab) = 22\n"
     ]
    }
   ],
   "source": [
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8d3243d5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab.freqs.keys()) = 1480\n",
      "len(edits.vocab.freqs.keys()) = 20 \n",
      "\n",
      "edits.vocab.freqs = Counter({'$keep': 2892, '$delete': 103, '$replace_,': 25, '$replace_.': 16, '$transform_agreement_singular': 15, '$append_.': 13, '$replace_the': 13, '$transform_verb_vbg_vb': 10, '$append_the': 9, '$transform_agreement_plural': 9, '$replace_to': 9, '$replace_of': 9, '$transform_verb_vbz_vb': 8, '$append_and': 7, '$append_a': 6, '$append_of': 6, '$append_to': 6, '$append_,': 6, '$replace_in': 6, '$transform_verb_vbn_vb': 3})\n"
     ]
    }
   ],
   "source": [
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {edits.vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ae29120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.vectors.shape = torch.Size([1482, 300])\n",
      "edits.vocab.vectors.shape = torch.Size([22, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens.vocab.vectors.shape}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits.vocab.vectors.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7484bbd",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9881b4ee",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        self.position_embedding_layer = nn.Embedding(max_lenght,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\") \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        \n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        position = torch.arange(0,num_of_tokens).unsqueeze(0)         # [1,num_of_tokens]\n",
    "        position = position.repeat(batch_size,1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.token_embedding_layer(tokens)\n",
    "        position_embeddings = self.position_embedding_layer(position)\n",
    "        # token_embeddings = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        # elementwise sum of tokens and position \n",
    "        combination = self.dropout(token_embeddings+position_embeddings)\n",
    "        # combination = [batch_size, num_of_tokens,embedding dimension]\n",
    "        #print(f\"batch_size, num_of_tokens,embedding dimension = {combination.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conved = combination.reshape(batch_size,-1).unsqueeze(1)\n",
    "        #conv_input = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        #print(f\"conv lay input = {conved.shape}\")\n",
    "        \n",
    "        # multiple convolution and activation layers\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv_layer(self.dropout(conved))\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "            \n",
    "            #pass through relu activation function\n",
    "            conved = F.relu(conved)*self.scale\n",
    "\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        \n",
    "        #print(f\"conv lay output = {conved.shape}\")\n",
    "        \n",
    "        conved = conved.squeeze(1).reshape(batch_size,num_of_tokens,-1)\n",
    "        #conved = [batch_size, num_of_tokens, embedding_dimension]\n",
    "        \n",
    "        \n",
    "        output = self.final_layer(conved+combination)*self.scale\n",
    "        #output = [batch size, num_of_tokens, target dimension]\n",
    "        #print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "995e0285",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\",device=self.device) \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(num_of_filters*embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "     def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.dropout(self.token_embedding_layer(tokens))\n",
    "        # token_embeddings = [batch_size, num_of_tokens, embedding dimension]\n",
    "        # print(f\"batch_size, num_of_tokens,embedding_dimension = {token_embeddings.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conv_input = token_embeddings.reshape(batch_size,-1).unsqueeze(1)\n",
    "        # conv_input = [batch size, 1, embedding_dimension*num_of_tokens]\n",
    "        # print(f\"conv lay input = {conv_input.shape}\")\n",
    "        \n",
    "        conved = [conv_layer(conv_input).squeeze(1) for conv_layer in self.conv_layers]\n",
    "        \n",
    "        cat = self.dropout(F.relu(torch.cat(conved, dim = 1)))\n",
    "        # [batch size, embedding dimension*num_of_tokens*num_of_filter]\n",
    "\n",
    "        conved = cat.reshape(batch_size,num_of_tokens,-1)\n",
    "        # conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "        # print(f\"conved before output = {conved.shape}\")\n",
    "        \n",
    "        output = self.final_layer(conved)*self.scale\n",
    "        # output = [batch size, num_of_tokens, target dimension]\n",
    "        # print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad412d2",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b07bcb53",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "clip = 0.1\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "EMBEDDING_DIM = tokens.vocab.vectors.shape[1]   #vocabular size, dim\n",
    "FILTER_SIZES = [1,3,5,7]\n",
    "N_FILTERS = len(FILTER_SIZES)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074921ca",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c38ca13d",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq_to_edits(INPUT_DIM,OUTPUT_DIM,OUTPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,DROPOUT,PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9fa205d5",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 481,242 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "862f3c77",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0104, -0.1829,  0.0761,  ..., -0.1362, -0.2240, -0.0552],\n",
       "        ...,\n",
       "        [-0.1169, -0.2692,  0.0215,  ..., -0.8629,  0.1006, -0.1516],\n",
       "        [-0.2858,  0.8411, -0.3313,  ..., -0.7876,  0.1089,  0.3702],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-tained embeddings \n",
    "\n",
    "pretrained_embeddings = tokens.vocab.vectors\n",
    "\n",
    "model.token_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6c21b7ae",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "\n",
    "model.token_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.token_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "40d081c6",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9f7907ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq_to_edits(\n",
       "  (token_embedding_layer): Embedding(1482, 300)\n",
       "  (position_embedding_layer): Embedding(100, 300)\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(1, 1, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (3): Conv1d(1, 1, kernel_size=(7,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=300, out_features=22, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0798e",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "80f71b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,data_iterator,optimizer,criterion,clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "        \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        \n",
    "        # Make the gradient vector to zero so \n",
    "        #So not to added with the new gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        # get output from the model\n",
    "        outputs = model(tokens_list)\n",
    "        # outputs = [batch size, num_of_tokens, target dimension]\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "        # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "        trg = batch.edits.contiguous().view(-1)\n",
    "        # trg = [batch size*num_of_tokens]\n",
    "        # print(f\"out dim = {outputs_1.shape}\")\n",
    "        \n",
    "        loss = criterion(outputs, trg)\n",
    "\n",
    "        # Backward and optimize\n",
    "        \n",
    "        # to calculate gradient\n",
    "        loss.backward()\n",
    "        #to avoid exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # to make the updates in the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        predicted = torch.argmax(outputs, dim=-1)\n",
    "        # print(f\"pre = {predicted.shape}\")\n",
    "        # print(f\"edit = {trg.shape}\")\n",
    "        \n",
    "        acc += accuracy_score(predicted, trg).item()\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "921e2892",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    testing = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            # converting to cpu or gpu variable\n",
    "            tokens_list = batch.tokens.to(device)\n",
    "            edits_list = batch.edits.to(device)\n",
    "            \n",
    "            outputs = model(tokens_list)\n",
    "            # outputs = [batch size, num_of_tokens, target_dimension]\n",
    "            \n",
    "            bact_size,num_tokens,output_dim = outputs.shape\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, output_dim)\n",
    "            # outputs = [batch size*num_of_tokens,embedding dimension]\n",
    "            trg = edits_list.contiguous().view(-1)\n",
    "            # trg = [batch size*num_of_tokens]\n",
    "            loss = criterion(outputs, trg)\n",
    "            epoch_loss += loss.item() \n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "#             print(f\"pre = {predicted.shape}\")\n",
    "#             print(f\"edit = {edits_list.shape}\")\n",
    "            acc += accuracy_score(predicted, trg).item()\n",
    "    \n",
    "            testing.append((predicted.reshape(bact_size,num_tokens),edits_list))\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc,testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddc3b",
   "metadata": {},
   "source": [
    "#### actual train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "82b0f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2223181f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.100, Train Accuracy: 97.098\n",
      "Epoch [2/50], Train Loss: 0.101, Train Accuracy: 97.130\n",
      "Epoch [3/50], Train Loss: 0.100, Train Accuracy: 97.066\n",
      "Epoch [4/50], Train Loss: 0.100, Train Accuracy: 97.130\n",
      "Epoch [5/50], Train Loss: 0.099, Train Accuracy: 97.034\n",
      "Epoch [6/50], Train Loss: 0.099, Train Accuracy: 96.987\n",
      "Epoch [7/50], Train Loss: 0.100, Train Accuracy: 97.066\n",
      "Epoch [8/50], Train Loss: 0.100, Train Accuracy: 96.955\n",
      "Epoch [9/50], Train Loss: 0.097, Train Accuracy: 97.018\n",
      "Epoch [10/50], Train Loss: 0.095, Train Accuracy: 97.258\n",
      "Epoch [11/50], Train Loss: 0.091, Train Accuracy: 97.353\n",
      "Epoch [12/50], Train Loss: 0.093, Train Accuracy: 97.226\n",
      "Epoch [13/50], Train Loss: 0.093, Train Accuracy: 97.082\n",
      "Epoch [14/50], Train Loss: 0.094, Train Accuracy: 97.034\n",
      "Epoch [15/50], Train Loss: 0.095, Train Accuracy: 97.210\n",
      "Epoch [16/50], Train Loss: 0.094, Train Accuracy: 97.210\n",
      "Epoch [17/50], Train Loss: 0.093, Train Accuracy: 97.130\n",
      "Epoch [18/50], Train Loss: 0.093, Train Accuracy: 97.178\n",
      "Epoch [19/50], Train Loss: 0.087, Train Accuracy: 97.305\n",
      "Epoch [20/50], Train Loss: 0.089, Train Accuracy: 97.369\n",
      "Epoch [21/50], Train Loss: 0.087, Train Accuracy: 97.258\n",
      "Epoch [22/50], Train Loss: 0.090, Train Accuracy: 97.353\n",
      "Epoch [23/50], Train Loss: 0.094, Train Accuracy: 97.194\n",
      "Epoch [24/50], Train Loss: 0.089, Train Accuracy: 97.290\n",
      "Epoch [25/50], Train Loss: 0.088, Train Accuracy: 97.290\n",
      "Epoch [26/50], Train Loss: 0.095, Train Accuracy: 97.162\n",
      "Epoch [27/50], Train Loss: 0.089, Train Accuracy: 97.178\n",
      "Epoch [28/50], Train Loss: 0.090, Train Accuracy: 97.305\n",
      "Epoch [29/50], Train Loss: 0.091, Train Accuracy: 97.242\n",
      "Epoch [30/50], Train Loss: 0.088, Train Accuracy: 97.290\n",
      "Epoch [31/50], Train Loss: 0.088, Train Accuracy: 97.242\n",
      "Epoch [32/50], Train Loss: 0.089, Train Accuracy: 97.098\n",
      "Epoch [33/50], Train Loss: 0.084, Train Accuracy: 97.290\n",
      "Epoch [34/50], Train Loss: 0.087, Train Accuracy: 97.210\n",
      "Epoch [35/50], Train Loss: 0.085, Train Accuracy: 97.369\n",
      "Epoch [36/50], Train Loss: 0.087, Train Accuracy: 97.417\n",
      "Epoch [37/50], Train Loss: 0.084, Train Accuracy: 97.433\n",
      "Epoch [38/50], Train Loss: 0.082, Train Accuracy: 97.226\n",
      "Epoch [39/50], Train Loss: 0.088, Train Accuracy: 97.210\n",
      "Epoch [40/50], Train Loss: 0.084, Train Accuracy: 97.385\n",
      "Epoch [41/50], Train Loss: 0.082, Train Accuracy: 97.433\n",
      "Epoch [42/50], Train Loss: 0.084, Train Accuracy: 97.369\n",
      "Epoch [43/50], Train Loss: 0.083, Train Accuracy: 97.353\n",
      "Epoch [44/50], Train Loss: 0.090, Train Accuracy: 97.305\n",
      "Epoch [45/50], Train Loss: 0.085, Train Accuracy: 97.146\n",
      "Epoch [46/50], Train Loss: 0.087, Train Accuracy: 97.130\n",
      "Epoch [47/50], Train Loss: 0.082, Train Accuracy: 97.385\n",
      "Epoch [48/50], Train Loss: 0.085, Train Accuracy: 97.226\n",
      "Epoch [49/50], Train Loss: 0.082, Train Accuracy: 97.385\n",
      "Epoch [50/50], Train Loss: 0.083, Train Accuracy: 97.226\n"
     ]
    }
   ],
   "source": [
    "# start = time.perf_counter()\n",
    "\n",
    "# try:\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss,train_acc = train_model(model,train_data_iterator,optimizer,criterion,clip)\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}')\n",
    "    loss.append(train_loss)\n",
    "    accuracy.append(train_acc)\n",
    "    \n",
    "# finally:\n",
    "#     end = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0fc3ea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(loss)),loss)\n",
    "axs[1].plot(range(len(accuracy)),accuracy)\n",
    "\n",
    "plt.setp(axs[1], xlabel='num of epoch')\n",
    "\n",
    "plt.setp(axs[0], ylabel='loss')\n",
    "plt.setp(axs[1], ylabel='acc')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ebdbe939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('my_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b991dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.353 Test accuracy 95.146\n"
     ]
    }
   ],
   "source": [
    "test_loss,accuracy,testing = evaluate_model(model, test_data_iterator, criterion)\n",
    "print(f\"Test Loss {test_loss:.3f} Test accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "03eb355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing[0][0][5] == testing[0][1][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d700cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[25, 13, 14, 29, 13, 13, 13, 25, 13, 13, 25, 21, 13, 13, 29, 13,  2,  2,\n",
    "        13, 13, 13, 25]\n",
    "b=[2, 2, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e32a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70a859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185d75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c5387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n",
    "\n",
    "for batch in train_data_iterator:\n",
    "    x = emb_1(batch.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.permute(0,2,1).reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 1)(x_input)\n",
    "b = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 2)(a)\n",
    "c = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 6)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.squeeze(1).reshape(2,5,-1)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f = {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = nn.Conv1d(in_channels =1 ,out_channels= 4,kernel_size = 3)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"g = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcc818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f9673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48fbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9f9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464793ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4972405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act = torch.Tensor([[2,0,1],[2,0,1]])\n",
    "y_pred = torch.argmax(torch.Tensor([[[0.1,3,5],[3,2,1],[0,1,0]],[[0.1,3,5],[3,2,1],[0,1,0]]]),dim=1)\n",
    "y_pred_1 = torch.argmax(torch.Tensor([[[1,5,2],[3,2,1],[0,1,0]],[[1,5,2],[3,2,1],[0,1,0]]]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d27840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_1,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb892691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa505f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e81a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b626a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f25d69",
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 1,padding=\"same\")(x_input).squeeze(1)\n",
    "c1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 3,padding=\"same\")(x_input).squeeze(1)\n",
    "d1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 5,padding=\"same\")(x_input).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.cat((b1,c1,d1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6a696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5da08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9cd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.arange(0,10,1)\n",
    "y1 = np.random.randn(10)\n",
    "y3 = np.random.randn(10)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, sharex='col')\n",
    "ax[0].plot(x,y1)\n",
    "ax[1].plot(x,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8b8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9279fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35753d30",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.sqrt(torch.FloatTensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conv 1D layers \n",
    "conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                            out_channels= 1,\n",
    "                                            kernel_size = filter_size,\n",
    "                                            padding = \"same\") \n",
    "                                  for filter_size in [1,3,5]])\n",
    "\n",
    "final_layer = nn.Linear(5*3, 3)\n",
    "\n",
    "# Dropout layers\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "# token_embeddings = [2, 1, 4*5]\n",
    "\n",
    "conved = [F.relu(conv_layer(x_input)).squeeze(1) for conv_layer in conv_layers]\n",
    "print(len(conved))\n",
    "concat = dropout(torch.cat(conved, dim = 1))\n",
    "# [batch size, embedding dimension*num_of_tokens*num_of_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conved = cat.reshape(2,4,-1)\n",
    "# conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "conved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = final_layer(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a914c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd4dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e8b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc0fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbcbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ed987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2c4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ceabb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn3]",
   "language": "python",
   "name": "conda-env-nn3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "265px",
    "width": "201px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1d36ce9c6112765ad20cbf471b33373de7b9da6b2d9a2b18d6540897511eb08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
