{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "\n",
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from seqeval.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc466f42",
   "metadata": {},
   "source": [
    "##### instal packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "210fb5dc",
   "metadata": {},
   "source": [
    "!conda install pytorch=1.9 torchvision torchaudio -c pytorch\n",
    "!conda install -y torchtext==0.10 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cba2cf",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #1,2,4,8,16,32,64,128,256,512,1028\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4c24",
   "metadata": {},
   "source": [
    "#### seed initializing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fbcf47e",
   "metadata": {},
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "torch.backends.cudnn.determininistic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space\n",
    "3. Build a vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "# removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"0\")\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"0\")\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(path='data_filter',train='ptrain.jsonl',test='val.jsonl',\n",
    "                                              format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')\n",
    "edits.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "sort_by_len = lambda x:len(x)\n",
    "\n",
    "train_data_iterator = BucketIterator(train_data,train=True,\n",
    "                                            batch_size=BATCH_SIZE, device= device)#,sort_within_batch=False)\n",
    "\n",
    "test_data_iterator =BucketIterator(test_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e5f1",
   "metadata": {},
   "source": [
    "#### data processing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e97f75e0",
   "metadata": {
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7feffacb9710>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['alistair', 'darling', 'is', 'expected', 'to', 'announce', 'details', 'of', 'tax', 'cuts', 'and', 'plans', 'to', 'increases', 'public', 'spending'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$append_.']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "518a8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = vars(test_data.examples[0])    # to print tokens of any row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7074d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 =len(test_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "732ffc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b37aebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 41])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 40])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 41])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 41])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 41])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 41])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 41])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 38])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 45])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 43])\n",
      "torch.Size([64, 44])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 42])\n",
      "torch.Size([64, 45])\n",
      "torch.Size([64, 48])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n",
      "torch.Size([64, 47])\n",
      "torch.Size([9, 41])\n"
     ]
    }
   ],
   "source": [
    "for i in test_data_iterator:\n",
    "    print(i.edits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d1c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ef8b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['$$$'] = 0\n",
      "tokens.vocab.itos[0] = <unk>\n",
      "edits.vocab.stoi['$keep'] = 2\n",
      "edits.vocab.itos[2] = $transform_verb_vbz_vb\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['$$$'] = {tokens.vocab.stoi['$$$']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[0]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['$keep']}\")\n",
    "print(f\"edits.vocab.itos[2] = {edits.vocab.itos[13]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "547693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab) = 65707\n",
      "len(edits.vocab) = 22\n"
     ]
    }
   ],
   "source": [
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d3243d5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab.freqs.keys()) = 65706\n",
      "len(edits.vocab.freqs.keys()) = 20 \n",
      "\n",
      "edits.vocab.freqs = Counter({'$keep': 1141063, '$delete': 41328, '$replace_.': 7600, '$replace_,': 7428, '$transform_agreement_singular': 6525, '$append_.': 5433, '$append_,': 5069, '$append_the': 4881, '$replace_to': 3771, '$replace_the': 3716, '$replace_of': 3586, '$transform_verb_vbz_vb': 3406, '$replace_in': 2999, '$transform_verb_vbg_vb': 2841, '$transform_verb_vbn_vb': 2757, '$append_to': 2612, '$append_of': 2500, '$transform_agreement_plural': 2442, '$append_and': 2370, '$append_a': 2283})\n"
     ]
    }
   ],
   "source": [
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {edits.vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae29120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.vectors.shape = torch.Size([65707, 100])\n",
      "edits.vocab.vectors.shape = torch.Size([22, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens.vocab.vectors.shape}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits.vocab.vectors.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7484bbd",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9881b4ee",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        self.position_embedding_layer = nn.Embedding(max_lenght,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\") \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        \n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        position = torch.arange(0,num_of_tokens).unsqueeze(0)         # [1,num_of_tokens]\n",
    "        position = position.repeat(batch_size,1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.token_embedding_layer(tokens)\n",
    "        # position_embeddings = self.position_embedding_layer(position)\n",
    "        # token_embeddings = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        # elementwise sum of tokens and position \n",
    "        combination = self.dropout(token_embeddings)#+position_embeddings)\n",
    "        # combination = [batch_size, num_of_tokens,embedding dimension]\n",
    "        #print(f\"batch_size, num_of_tokens,embedding dimension = {combination.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conved = combination.reshape(batch_size,-1).unsqueeze(1)\n",
    "        #conv_input = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        #print(f\"conv lay input = {conved.shape}\")\n",
    "        \n",
    "        # multiple convolution and activation layers\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv_layer(self.dropout(conved))\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "            \n",
    "            #pass through relu activation function\n",
    "            conved = F.relu(conved)*self.scale\n",
    "\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        \n",
    "        #print(f\"conv lay output = {conved.shape}\")\n",
    "        \n",
    "        conved = conved.squeeze(1).reshape(batch_size,num_of_tokens,-1)\n",
    "        #conved = [batch_size, num_of_tokens, embedding_dimension]\n",
    "        \n",
    "        \n",
    "        output = self.final_layer(conved+combination)*self.scale\n",
    "        #output = [batch size, num_of_tokens, target dimension]\n",
    "        #print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b52face",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\",device=self.device) \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(num_of_filters*embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "     def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.dropout(self.token_embedding_layer(tokens))\n",
    "        # token_embeddings = [batch_size, num_of_tokens, embedding dimension]\n",
    "        # print(f\"batch_size, num_of_tokens,embedding_dimension = {token_embeddings.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conv_input = token_embeddings.reshape(batch_size,-1).unsqueeze(1)\n",
    "        # conv_input = [batch size, 1, embedding_dimension*num_of_tokens]\n",
    "        # print(f\"conv lay input = {conv_input.shape}\")\n",
    "        \n",
    "        conved = [conv_layer(conv_input).squeeze(1) for conv_layer in self.conv_layers]\n",
    "        \n",
    "        cat = self.dropout(F.relu(torch.cat(conved, dim = 1)))\n",
    "        # [batch size, embedding dimension*num_of_tokens*num_of_filter]\n",
    "\n",
    "        conved = cat.reshape(batch_size,num_of_tokens,-1)\n",
    "        # conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "        # print(f\"conved before output = {conved.shape}\")\n",
    "        \n",
    "        output = self.final_layer(conved)*self.scale\n",
    "        # output = [batch size, num_of_tokens, target dimension]\n",
    "        # print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad412d2",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b07bcb53",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 75\n",
    "clip = 0.1\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "FILTER_SIZES = [1,2,3]\n",
    "N_FILTERS = len(FILTER_SIZES)\n",
    "DROPOUT = 0.75\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074921ca",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c38ca13d",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq_to_edits(INPUT_DIM,OUTPUT_DIM,OUTPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,DROPOUT,PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fa205d5",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,582,931 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "862f3c77",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.6149,  0.9273,  0.5583,  ..., -0.0922, -0.0798, -1.2843],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-tained embeddings \n",
    "\n",
    "pretrained_embeddings = tokens.vocab.vectors\n",
    "\n",
    "model.token_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c21b7ae",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "\n",
    "model.token_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.token_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40d081c6",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f7907ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq_to_edits(\n",
       "  (token_embedding_layer): Embedding(65707, 100)\n",
       "  (position_embedding_layer): Embedding(100, 100)\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(1, 1, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(1, 1, kernel_size=(2,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=100, out_features=22, bias=True)\n",
       "  (dropout): Dropout(p=0.75, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0798e",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80f71b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,data_iterator,optimizer,criterion,clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "        \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        \n",
    "        # Make the gradient vector to zero so \n",
    "        #So not to added with the new gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        # get output from the model\n",
    "        outputs = model(tokens_list)\n",
    "        # outputs = [batch size, num_of_tokens, target dimension]\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "        # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "        trg = batch.edits.contiguous().view(-1)\n",
    "        # trg = [batch size*num_of_tokens]\n",
    "        # print(f\"out dim = {outputs_1.shape}\")\n",
    "        \n",
    "        loss = criterion(outputs, trg)\n",
    "\n",
    "        # Backward and optimize\n",
    "        \n",
    "        # to calculate gradient\n",
    "        loss.backward()\n",
    "        #to avoid exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # to make the updates in the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        predicted = torch.argmax(outputs, dim=-1)\n",
    "        # print(f\"pre = {predicted.shape}\")\n",
    "        # print(f\"edit = {trg.shape}\")\n",
    "        \n",
    "        acc += accuracy_score(predicted, trg).item()\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "921e2892",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            # converting to cpu or gpu variable\n",
    "            tokens_list = batch.tokens.to(device)\n",
    "            edits_list = batch.edits.to(device)\n",
    "            \n",
    "            outputs = model(tokens_list)\n",
    "            # outputs = [batch size, num_of_tokens, embedding dimension]\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "            # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "            trg = batch.edits.contiguous().view(-1)\n",
    "            # trg = [batch size*num_of_tokens]\n",
    "            loss = criterion(outputs, trg)\n",
    "            epoch_loss += loss.item() \n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "            # print(f\"pre = {predicted.shape}\")\n",
    "            # print(f\"edit = {edits_list.shape}\")\n",
    "\n",
    "            acc += accuracy_score(predicted, trg).item()\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddc3b",
   "metadata": {},
   "source": [
    "#### actual train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e3e4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2223181f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Train Loss: 0.260, Train Accuracy: 94.877\n",
      "Epoch [2/75], Train Loss: 0.215, Train Accuracy: 95.469\n",
      "Epoch [3/75], Train Loss: 0.209, Train Accuracy: 95.497\n",
      "Epoch [4/75], Train Loss: 0.206, Train Accuracy: 95.516\n",
      "Epoch [5/75], Train Loss: 0.204, Train Accuracy: 95.517\n",
      "Epoch [6/75], Train Loss: 0.202, Train Accuracy: 95.525\n",
      "Epoch [7/75], Train Loss: 0.200, Train Accuracy: 95.542\n",
      "Epoch [8/75], Train Loss: 0.199, Train Accuracy: 95.550\n",
      "Epoch [9/75], Train Loss: 0.198, Train Accuracy: 95.562\n",
      "Epoch [10/75], Train Loss: 0.198, Train Accuracy: 95.554\n",
      "Epoch [11/75], Train Loss: 0.197, Train Accuracy: 95.560\n",
      "Epoch [12/75], Train Loss: 0.197, Train Accuracy: 95.562\n",
      "Epoch [13/75], Train Loss: 0.197, Train Accuracy: 95.559\n",
      "Epoch [14/75], Train Loss: 0.196, Train Accuracy: 95.565\n",
      "Epoch [15/75], Train Loss: 0.196, Train Accuracy: 95.574\n",
      "Epoch [16/75], Train Loss: 0.196, Train Accuracy: 95.580\n",
      "Epoch [17/75], Train Loss: 0.196, Train Accuracy: 95.577\n",
      "Epoch [18/75], Train Loss: 0.196, Train Accuracy: 95.582\n",
      "Epoch [19/75], Train Loss: 0.196, Train Accuracy: 95.585\n",
      "Epoch [20/75], Train Loss: 0.195, Train Accuracy: 95.588\n",
      "Epoch [21/75], Train Loss: 0.195, Train Accuracy: 95.591\n",
      "Epoch [22/75], Train Loss: 0.195, Train Accuracy: 95.586\n",
      "Epoch [23/75], Train Loss: 0.195, Train Accuracy: 95.590\n",
      "Epoch [24/75], Train Loss: 0.195, Train Accuracy: 95.596\n",
      "Epoch [25/75], Train Loss: 0.195, Train Accuracy: 95.597\n",
      "Epoch [26/75], Train Loss: 0.195, Train Accuracy: 95.590\n",
      "Epoch [27/75], Train Loss: 0.195, Train Accuracy: 95.599\n",
      "Epoch [28/75], Train Loss: 0.194, Train Accuracy: 95.599\n",
      "Epoch [29/75], Train Loss: 0.194, Train Accuracy: 95.606\n",
      "Epoch [30/75], Train Loss: 0.194, Train Accuracy: 95.606\n",
      "Epoch [31/75], Train Loss: 0.194, Train Accuracy: 95.593\n",
      "Epoch [32/75], Train Loss: 0.194, Train Accuracy: 95.608\n",
      "Epoch [33/75], Train Loss: 0.195, Train Accuracy: 95.595\n",
      "Epoch [34/75], Train Loss: 0.194, Train Accuracy: 95.595\n",
      "Epoch [35/75], Train Loss: 0.195, Train Accuracy: 95.591\n",
      "Epoch [36/75], Train Loss: 0.194, Train Accuracy: 95.598\n",
      "Epoch [37/75], Train Loss: 0.194, Train Accuracy: 95.608\n",
      "Epoch [38/75], Train Loss: 0.195, Train Accuracy: 95.591\n",
      "Epoch [39/75], Train Loss: 0.194, Train Accuracy: 95.595\n",
      "Epoch [40/75], Train Loss: 0.194, Train Accuracy: 95.602\n",
      "Epoch [41/75], Train Loss: 0.194, Train Accuracy: 95.600\n",
      "Epoch [42/75], Train Loss: 0.194, Train Accuracy: 95.591\n",
      "Epoch [43/75], Train Loss: 0.194, Train Accuracy: 95.604\n",
      "Epoch [44/75], Train Loss: 0.195, Train Accuracy: 95.591\n",
      "Epoch [45/75], Train Loss: 0.194, Train Accuracy: 95.600\n",
      "Epoch [46/75], Train Loss: 0.194, Train Accuracy: 95.606\n",
      "Epoch [47/75], Train Loss: 0.194, Train Accuracy: 95.598\n",
      "Epoch [48/75], Train Loss: 0.194, Train Accuracy: 95.601\n",
      "Epoch [49/75], Train Loss: 0.194, Train Accuracy: 95.600\n",
      "Epoch [50/75], Train Loss: 0.194, Train Accuracy: 95.595\n",
      "Epoch [51/75], Train Loss: 0.194, Train Accuracy: 95.592\n",
      "Epoch [52/75], Train Loss: 0.194, Train Accuracy: 95.597\n",
      "Epoch [53/75], Train Loss: 0.194, Train Accuracy: 95.599\n",
      "Epoch [54/75], Train Loss: 0.194, Train Accuracy: 95.609\n",
      "Epoch [55/75], Train Loss: 0.194, Train Accuracy: 95.598\n",
      "Epoch [56/75], Train Loss: 0.194, Train Accuracy: 95.598\n",
      "Epoch [57/75], Train Loss: 0.194, Train Accuracy: 95.599\n",
      "Epoch [58/75], Train Loss: 0.194, Train Accuracy: 95.599\n",
      "Epoch [59/75], Train Loss: 0.194, Train Accuracy: 95.596\n",
      "Epoch [60/75], Train Loss: 0.194, Train Accuracy: 95.598\n",
      "Epoch [61/75], Train Loss: 0.194, Train Accuracy: 95.599\n",
      "Epoch [62/75], Train Loss: 0.194, Train Accuracy: 95.605\n",
      "Epoch [63/75], Train Loss: 0.195, Train Accuracy: 95.587\n",
      "Epoch [64/75], Train Loss: 0.194, Train Accuracy: 95.594\n",
      "Epoch [65/75], Train Loss: 0.195, Train Accuracy: 95.588\n",
      "Epoch [66/75], Train Loss: 0.194, Train Accuracy: 95.587\n",
      "Epoch [67/75], Train Loss: 0.194, Train Accuracy: 95.606\n",
      "Epoch [68/75], Train Loss: 0.194, Train Accuracy: 95.610\n",
      "Epoch [69/75], Train Loss: 0.195, Train Accuracy: 95.592\n",
      "Epoch [70/75], Train Loss: 0.194, Train Accuracy: 95.597\n",
      "Epoch [71/75], Train Loss: 0.195, Train Accuracy: 95.593\n",
      "Epoch [72/75], Train Loss: 0.194, Train Accuracy: 95.602\n",
      "Epoch [73/75], Train Loss: 0.194, Train Accuracy: 95.592\n",
      "Epoch [74/75], Train Loss: 0.194, Train Accuracy: 95.604\n",
      "Epoch [75/75], Train Loss: 0.194, Train Accuracy: 95.599\n"
     ]
    }
   ],
   "source": [
    "# start = time.perf_counter()\n",
    "\n",
    "# try:\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss,train_acc = train_model(model,train_data_iterator,optimizer,criterion,clip)\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}')\n",
    "    loss.append(train_loss)\n",
    "    accuracy.append(train_acc)\n",
    "    \n",
    "# finally:\n",
    "#     end = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb752de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAy00lEQVR4nO3deZykZX3v/c+vtq7e91l7hlkYdmGAcQBBIuACPC7RaJQoIUZFE/HAiecoJs9jzDnPOdHELD4nqAcBkxiPGneiCBIUNwIyAwMyGwyz9qzdPb1Xd62/54/77p6anu7p7pnuqZrp7/v1qlfVfd1L/bqm5v7VdV33fV3m7oiIiExVpNQBiIjI6UWJQ0REpkWJQ0REpkWJQ0REpkWJQ0REpiVW6gBOhZaWFl+2bFmpwxAROa2sX7++091bx5aXJHGY2Y3A54AocJ+7f3rM+ncDHw8XB4A/cvfnwnUNwH3ARYADf+ju/3G891u2bBnr1q2b0b9BRORMZ2a7xis/5YnDzKLAPcDrgHbgaTN70N03FW22A/gtd+82s5uAe4ErwnWfAx5297ebWQKoOoXhi4jMeaXo41gLbHP37e6eAb4OvKV4A3d/wt27w8UngTYAM6sDrgXuD7fLuHvPqQpcRERKkzgWA3uKltvDsom8D/hR+HoF0AF82cyeNbP7zKx6dsKEv3p4C2/4u5/P1uFFRE5LpUgcNk7ZuOOemNl1BIljpL8jBlwGfMHdLwUGgbsn2Pd2M1tnZus6OjpOKNBcwdnZNXhC+4qInKlKkTjagSVFy23AvrEbmdnFBJ3gb3H3rqJ92939qXD5WwSJ5Bjufq+7r3H3Na2tx1wUMCX1lXHSuQLD2fwJ7S8iciYqReJ4GlhlZsvDzu13AQ8Wb2BmS4HvALe6+4sj5e5+ANhjZueGRTcAxZ3qM6q+Mg5A31B2tt5CROS0c8qvqnL3nJndATxCcDnuA+6+0cw+FK7/IvBJoBn4vJkB5Nx9TXiIjwBfDZPOduC9sxXrSOLoGcoyry45W28jInJaKcl9HO7+EPDQmLIvFr1+P/D+CfbdAKwZb91MG0kcvapxiIiM0pAjx9FQFSaOlBKHiMgIJY7jUI1DRORYShzHUdzHISIiASWO46hNqsYhIjKWEsdxRCNGbTKmy3FFRIoocUyioSquGoeISBEljknUV8bpSWVKHYaISNlQ4phEfaVqHCIixZQ4JqHEISJyNCWOSdRXJugdypU6DBGRsqHEMYmgxpHBfdyR30VE5hwljknUV8bJ5p0hDa0uIgIocUxKw46IiBxNiWMSowMdKnGIiABKHJMaHa9KI+SKiABKHJNSU5WIyNGUOCahxCEicjQljknUV2necRGRYkock6hJxIiY+jhEREaUJHGY2Y1mttXMtpnZ3eOsf7eZPR8+njCzS8asj5rZs2b2g9mONRIx6jTsiIjIqFOeOMwsCtwD3ARcANxiZheM2WwH8FvufjHw34F7x6y/E9g827GO0HhVIiJHlKLGsRbY5u7b3T0DfB14S/EG7v6Eu3eHi08CbSPrzKwN+L+A+05RvDQocYiIjCpF4lgM7Clabg/LJvI+4EdFy38PfAwoHO9NzOx2M1tnZus6OjpOMNRAXWVc846LiIRKkThsnLJxRxA0s+sIEsfHw+U3Aofcff1kb+Lu97r7Gndf09raejLxUl8Z11VVIiKhWAnesx1YUrTcBuwbu5GZXUzQHHWTu3eFxVcDbzazm4EkUGdm/+Lu75nNgNXHISJyRClqHE8Dq8xsuZklgHcBDxZvYGZLge8At7r7iyPl7v4Jd29z92Xhfj+Z7aQBR+Yd19DqIiIlqHG4e87M7gAeAaLAA+6+0cw+FK7/IvBJoBn4vJkB5Nx9zamOdUR9ZZx8wRlI56hNxksVhohIWShFUxXu/hDw0JiyLxa9fj/w/kmO8Tjw+CyEd4ziYUeUOERkrtOd41Og8apERI5Q4piC+soEoMQhIgJKHFMyWuPQeFUiIkocU1GvWQBFREYpcUyB+jhERI446cRhZneaWZ0F7jezZ8zs9TMRXLmoTkSJRUyJQ0SEmalx/KG79wGvB1qB9wKfnoHjlg0zo17jVYmIADOTOEbGnroZ+LK7P8f441Gd1jTsiIhIYCYSx3oz+zFB4njEzGqZZOTa01GdBjoUEQFm5s7x9wGrge3unjKzJoLmqjNKfWWc7lSm1GGIiJTcTNQ4rgK2unuPmb0H+L+B3hk4bllpqIpr3nEREWYmcXwBSIXzgn8M2AX88wwct6yoj0NEJDATiSPnwXjjbwE+5+6fA2pn4Lhlpb4yTt9wlkJBQ6uLyNw2E4mj38w+AdwK/NDMosAZN4RsfWUcd+hP50odiohISc1E4ngnkCa4n+MAwfzhfz0Dxy0rGq9KRCRw0okjTBZfBerDOcGH3f2M7OMADTsiIjITQ478LvBr4B3A7wJPmdnbT/a45UaJQ0QkMBP3cfwZ8Ep3PwRgZq3AvwPfmoFjlw2NkCsiEpiJPo7ISNIIdU12XDO70cy2mtk2M7t7nPXvNrPnw8cT4aW+mNkSM/upmW02s41mducMxD8lDeFkTj1DuglQROa2mahxPGxmjwBfC5ffyZj5xIuFV13dA7wOaAeeNrMH3X1T0WY7gN9y924zuwm4F7gCyAEfdfdnwqFN1pvZo2P2nRVqqhIRCZx04nD3/2pmvwNcTTC44b3u/t3j7LIW2Obu2wHM7OsE94CMnvzd/Ymi7Z8E2sLy/cD+8HW/mW0muIpr1hNHMh4hEY0ocYjInDcTNQ7c/dvAt6e4+WJgT9FyO0FtYiLvA340ttDMlgGXAk+Nt5OZ3Q7cDrB06dIphjYxM9NAhyIinETiMLN+YLzbqA1wd6+baNdxysa9HdvMriNIHNeMKa8hSFR3hXOBHHtA93sJmrhYs2bNjNzurfGqREROInG4+4kOK9IOLClabgP2jd3IzC4G7gNucveuovI4QdL4qrt/5wRjOCEar0pEpDRzjj8NrDKz5WaWAN4FPFi8gZktBb4D3OruLxaVG3A/sNnd//YUxgwocYiIQAkSh7vngDuAR4DNwL+6+0Yz+5CZfSjc7JNAM/B5M9tgZuvC8qsJxsS6PizfYGY3n6rYlThERGaoc3y63P0hxlyy6+5fLHr9fuD94+z3S0o4LW19ZVxjVYnInFeKpqrTVn1lnP50jqFMvtShiIiUjBLHNFx9dgsAX31qV4kjEREpHSWOaVi7vIlrzm7hC4+/zKDm5RCROUqJY5r+5PXn0DWY4R+f2FnqUERESkKJY5ouW9rI9efN496fb6dvWB3lIjL3KHGcgD953Tn0DmV54Jc7Sh2KiMgpp8RxAi5aXM8bLpzP/b/YQU9Kw6yLyNyixHGC/vPrzmEgk+Pen28vdSgiIqeUEscJOm9BHW+8eBH/+MROdnYOljocEZFTRonjJHzsDeeSjEe57cu/pqM/XepwREROCSWOk7CkqYr7b1vDob40f/iPTzOgeztEZA5Q4jhJly5t5J53X8qm/X380b+sJ5MrlDokEZFZpcQxA64/bz5/+bZX8IuXOvnYt56jUJiReaNERMpSSUbHPRP97poldPSn+etHttIxkOZv3rGaBfXJUoclIjLjVOOYQX/8mpX85dtewTO7erjxcz/nkY0HSh2SiMiMU+KYQWbGLWuX8oP/dA1tjZV88Cvr+cR3fqM5PETkjKLEMQtWttbwnT+6mg/+1gq+/vRurvzLx/jk919gh+73EJEzgLmf+R25a9as8XXr1k2+4SzYvL+P+3+5gwc37CNbKHDDefP4z687hwsX1ZckHhGRqTKz9e6+Zmx5SWocZnajmW01s21mdvc4699tZs+HjyfM7JKp7ltuzl9Yx2ffcQm/vPs6PnL9Kp7Z3cOb/+FXfPpHWxjOaiZBETn9nPIah5lFgReB1wHtwNPALe6+qWibVwGb3b3bzG4CPuXuV0xl3/GUssYxVm8qy/98aDPfWLeHZc1V/M+3vYJXrWwpdVgiIscopxrHWmCbu2939wzwdeAtxRu4+xPu3h0uPgm0TXXfcldfFeczb7+Y//P+K3Dg9770FO+57yke+OUOjXklIqeFUiSOxcCeouX2sGwi7wN+NN19zex2M1tnZus6OjpOItzZ8aqzW3j4zmu567Wr2N87xH/7wSZe89nHuf6zj/PpH21hy4G+UocoIjKuUtwAaOOUjdteZmbXESSOa6a7r7vfC9wLQVPV9MOcfZWJKHe99hzueu057O5K8ZMtB3lsyyG+9IvtfPFnL3PeglresnoxV5/dTG0yTnVFlNqKOMl4BLPxPgoRkdlXisTRDiwpWm4D9o3dyMwuBu4DbnL3runsezpa2lzFH1y9nD+4ejldA2l++Jv9fO/ZvXzm4S3HbNtYFee158/nxosWcM2qFipi0RJELCJzVSk6x2MEHdw3AHsJOrh/z903Fm2zFPgJ8Pvu/sR09h1POXWOT9furhRbDvQxmMkxkM4zMJxj64E+HttyiP7hHDUVMa5Y3kRtMkZFLEoyHqGqIsbylmpWzavh7Hk11Cbjpf4zROQ0NFHn+Cmvcbh7zszuAB4BosAD7r7RzD4Urv8i8EmgGfh82CSTc/c1E+17qv+GU2lpcxVLm6uOKc/kCvzq5U4eeeEAz+zuZjhbIJ3Lk84VGEznyOaP/CBYUJdkUUOSBfVJFtRVMr+ugqqKGBXRCBXxCIlohMpElJqKGFWJGDUVMWLRoCnMDAyjoSpOMq6ajYjoBsAzUr7g7Dmc4sWD/bx0aICXOwY40DvMgb5hDvQOk8pM//6RaMRYNa+GVyyu5xVt9cyvS9I/nKNvKEvfcJZc3mmoitNUnaCxOkFzdYIFdUmaayqIRtQfI3I6Kpsah8y+aMRY1lLNspZqXn/h0evcncFMnlQmRyZXIJ0rkM4WGMrmGUznSIVNYvlCAffgygN32NuT4jd7gyayb65vP+Y9IwbjjSYfjRjzaitoqamg4E4u72TzBQruVFfEqEvGqauMUZuMk4hFiEeMaCRCPGok41GqK6KjtSDHg5pVNs9wrkBlPMrihkoWN1bS1lg5601yw9k8ETMSsaMvRszmC+zqGmTboQHAuGBhHUuaKie9gCGbL3CoP01NRYzaihiRWUiwQ5k8eXdqKvRffaryBef59h729QyztCmo8ddXHv3dGs7mGcrkqU3GiEVP7OLUroE08ViEutOwKVnfpjnGzKipiJ3wicTd2dc7TPdgZvSkX1MRI2JGfzpH92CGw6kMXQOZsIYzxIHeNJ0DaaIRIx41YtEIETNS6Rx9w1l2daXoG8qSyTu5QmE0uaSnOSlWbUWM1tqK0UdTdYJkPEoiGqEiFiERixCLBkkpFolgBocHM3QOpOkcyNCTylBfGWdebZJ5dcH+e7uH2HKgjy0H+tnVlQKgLhmjpSZY3zOUZWfnILkxWbO2IsZ5C2tZ0VJDS22CpuoKmqsTFNx5vr2X59t72Livb/RvjBjUVcZprErQ1lgZnLCaqljSVEVlPEosakQjRsSM9u4hXu4YYNuhAXZ0DlIRi4wm0MUNlfQP59h6oJ8tB/rYdTiFO7TUVLCipZplLVUsaayiqSZBU1WCpuoE1RUxhrJ5Upk8Q5ncaEzRiBE1w4zgB0b4GM7k6RnKcHgwS/dghu5UhuFcgWyuQK5QIJsPEtW82grm1VXQWpvEgJ5Uhp6hLN2pLBWxCOfOr+WcBbWcO7+WmmSMl8Pa8bZDAxzqT1OViFIdflerwqbU6vB1ZTxKJl8glckznA0ehhELv1+xMPZIxIhGIGJGNu8MZfOkw5N+PBahsSpOQ1WChso4+3uHeXzrIX7+UieHBzNH/Xs2VsVpqamgfzhHz1CG4eyR72ZdMkZTdfBZLmuuZuW8Gla2Bj/c8gWnfzjHwHCO3qEsL3cMsGl/H5v29XEonG66qTrB0qYqljVXEY1EONQ/TEd/enQ66raR70JjJfWVcbpT2dH/Z+lcgSWNlSxvqeas5moW1icZTOfoTmXpHcrQk8py8ysWsqTp2Obuk6GmKilb+ULwHz2VzjGQzhExoyIeIRmLUhGPMJDOsbd7iL09Q+ztHmJ/7zAdA2k6+tJ0DKQ5PJgJa1X5cWtDI2oqYrTUJKivStCbynCoPz3anGcGy5urOW9hLefMryViRtdAmq7BIDnWJGOjFyGcPa+GfMHZvL+fzfv72Ly/j51dKbpTGfJFAVTGo1y0uI5XLG5g5bxqhjJ5eoey9A5l6RrI0N6dYtfhFD3HGVU5FtYqV7RUk8kXRj+HVCZPxGBZczXnLqjl3AW1JGIRdnYOsrMzxY6uwdET0smIRozGqgRN1UGyS8ajowk5GjX6h3PhyW+YrsEM7sEJtrE6QUNVgsF0jh2dg0d9LiPqK+MsrE+O1oIH0rmjTtSzqbk6wbXntPKac1tZ2VpDe/cQu7oG2XU4RddAmrpknIYw2VTGo/QNjyTPLB39aXZ2DbK/d3jC48cixtnzarhwUT3nL6wlX3B2dqWC9+hKUXBnXm2QbOfVVeAO7d0pdh9Osbd7iFzBiUctaBKuShCPRth9OEXv0MTflftvW8MN588/oc9DTVVy2olGjtSO5o2zvioRY15tkkuXNk56rFxYg8nlnUw++GXsDo1VCSoTx3b6D6RzdA2kmVebHHf98YyNp1Bw+oazdA1mKBSc5S3VU2re6B3Ksrd7iHQuT77gZPNOwZ0F9UmWNlURH3MMd6d3KEtFLHrcmDO5At1hrbA7lWEgnaMqEQ1/yceoiEdwh4IH71coQEU8qLVVhEm7tiI25XuJcvkCZnZMX1c6l2dH5yBbD/QzkM6xoiVIvi01iWOOncsXSIWJZDCdYyhToCIeoTIe/K3JeBQPm0JzhaDmmi8EsRfcybsTj0RIJoJ9kvEo2Xxh9Nd771CW2mSMixbVH9VkeNHi6Q9GOpDOsaNjkJ1dg8SjRm0yHjRHJmMsbqw84cvnc/kCw7kC1YnoMZ9PTyrDjs5BDvYNU5uMU18ZD5J0ZZyqaX5/p0I1DhERGVc5jVUlIiKnMSUOERGZljnRVGVmHcCuE9y9BeicwXBmg2KcOadDnIpxZijGyZ3l7q1jC+dE4jgZZrZuvDa+cqIYZ87pEKdinBmK8cSpqUpERKZFiUNERKZFiWNy95Y6gClQjDPndIhTMc4MxXiC1MchIiLTohqHiIhMixKHiIhMixLHcZjZjWa21cy2mdndpY4HwMweMLNDZvZCUVmTmT1qZi+Fz5MP3jS7MS4xs5+a2WYz22hmd5ZbnGaWNLNfm9lzYYx/UW4xFsUaNbNnzewH5Rijme00s9+Y2QYzW1emMTaY2bfMbEv4vbyqDGM8N/wMRx59ZnZXucUJShwTMrMocA9wE3ABcIuZXVDaqAD4R+DGMWV3A4+5+yrgsXC5lHLAR939fOBK4MPhZ1dOcaaB6939EmA1cKOZXUl5xTjiTmBz0XI5xnidu68uuueg3GL8HPCwu58HXELweZZVjO6+NfwMVwOXAyngu5RZnEAwoqYexz6Aq4BHipY/AXyi1HGFsSwDXiha3gosDF8vBLaWOsYx8X4feF25xglUAc8AV5RbjEAbwcnieuAH5fjvDewEWsaUlU2MQB2wg/BioHKMcZyYXw/8qlzjVI1jYouBPUXL7WFZOZrv7vsBwufxRiEvCTNbBlwKPEWZxRk2AW0ADgGPunvZxQj8PfAxoHhCinKL0YEfm9l6M7s9LCunGFcAHcCXwya/+8ysusxiHOtdwNfC12UXpxLHxMabbEDXLk+DmdUA3wbucve+UsczlrvnPWgWaAPWmtlFJQ7pKGb2RuCQu68vdSyTuNrdLyNo1v2wmV1b6oDGiAGXAV9w90uBQcqhuWcCZpYA3gx8s9SxTESJY2LtwJKi5TZgX4limcxBM1sIED4fKnE8mFmcIGl81d2/ExaXXZwA7t4DPE7Qd1ROMV4NvNnMdgJfB643s3+hvGLE3feFz4cI2uTXUl4xtgPtYY0S4FsEiaScYix2E/CMux8Ml8suTiWOiT0NrDKz5eEvgHcBD5Y4pok8CNwWvr6NoE+hZCyYnux+YLO7/23RqrKJ08xazawhfF0JvBbYQhnF6O6fcPc2d19G8P37ibu/hzKK0cyqzax25DVB2/wLlFGM7n4A2GNm54ZFNwCbKKMYx7iFI81UUI5xlrqTpZwfwM3Ai8DLwJ+VOp4wpq8B+4EswS+p9wHNBB2oL4XPTSWO8RqCZr3ngQ3h4+ZyihO4GHg2jPEF4JNhednEOCbe13Ckc7xsYiToP3gufGwc+X9STjGG8awG1oX/3t8DGsstxjDOKqALqC8qK7s4NeSIiIhMi5qqRERkWpQ4RERkWpQ4RERkWmKlDuBUaGlp8WXLlpU6DBGR08r69es7fZw5x+dE4li2bBnr1q0rdRgiIqcVM9s1XrmaqkREZFrmRI1D5ETkC07EILif8eQF18BDJDIzxztTdA6k6RvKsrylesY+63JRKDjdqQyH+tN0D2a4cHE99ZXxUod10pQ45Izh7kedeHpSGTbs6eHZ3T08u6eH7R0DAEQjRtSM6ooYN160gHdc3sa8uuToflsP9POlX2zn+xv2srK1hnesWcJvr15Ec03FtGPK5Ar8esdh/n3zQR7ddJDeoSy/f9VZvP/VK2iqTky4Xy5fYMuBfjbt76MiFqGpOkFjVYLG6gQL65LjJp+XDvbz7Wf2Up2I8juXt7GoofKYbVKZHBv39bHt0AAvHRxgW8cAA8NZLm5r4NKlDVy2tJG2xkqy+eCE1zmQpjeVJRIx4tEIFbEIyXiExQ1VVCai0/48Rmw7NDD6mTyzuxt3WNFazU0XLeCmixZy4aK6CZNIvuAMZfNEzYhFjVjEpp1w3J327iE27utj0/4+Nu3ro6k6zkeuX8WSpqoT/rsgSBbffXYv9zy+jT2HU2TzR+6Vq0vG+NBrVvLeVy2f1ueXyxd4rr2X7sEMV65spqbi6FN3JlfgFy91sG5XNwvrkyxvqWZZczWLGiqJzsIPlTlxA+CaNWtcfRyllS/4pF/g4WyeTfv72Li3l037++gbzpHPO7mCky8UmF+X5BVt9bxicT3nLqhlOFPgiZc7+cW2Tn7xUgd7Dg8BYBaMUFkIv9oRg3Pm13LegloiZuTdyRecA73DrNvVTTRiXH/ePF57/jwe+s0BfvZiB5XxKG+6ZCFbD/TzXHsv8WiwzbnzazEzohEjYtBQlaCtsZK2xiraGisZTOd4vr2X59p7eL69l6d3HKY/naMiFuGas1tIxCI8vPEAlfEot151FrdeeRZDmTz7eofZ3zPErsMpNuzu4bn2HlKZ/LifU3N1gitXNHPVymbWLm9iw54evvH0Htbv6iYWMXIFxwyuXdXKO1+5hPl1SX61rZNfbuvk2d3doyeyyniUlfOqqUrEeGFv7+j7VcajDGXHf+8RZrCksYqz59Vw9rwa5tclaayK01iVoL4qTk8qw47OFDs6B9jZmaJrMEMqk2MwnSeVyY2+14WL6njdBfNpqk7wyMYDPLn9MPmC01SdoDIeJRoJEgPAYCbHwHCOwXE+l1jEqKuM01AVp6EyTkNVgnzBR99rKJNnKJtnOJtnOFtgOJfHi74fy1uq2dszRKEA7716GX983dmjNYPhbJ6N+3o50Jvm7Hk1rGitJh4dv5X/qe1d/PcfbuKFvX1c3FbP1We3MK+2gvl1SSoTUb7yH7v4yZZDtNZW8JHrz+bc+bVs7xxke8cA2zsGcaCtsZLFDcF3qncoyy9e6uBX2zrpG84BkIhGuGJFEzecN4+zmqt5+IUDPLzxAL1DWcyg+JSeiEX437deznXnntiAuma23o/MsXKkXIlDTlQ6l6drIPhV2jmQprM/Q8dAmo7+YPlQf5rO/uB5IJ1jYX2Ss5qrRn8JDaRzdPQH2x/oG2ZH5yD58GzfWBWnqTpBLBIJTtIRaO8eoieVBSAeNfIFp+BQUxHjyhXNXLCwFsxGm4SqK2JcsqSei9sajvmFNmJH5yDfeHoP31rfTudAmpaaCt579TLefcVSGqqCGsHWA/18c90evrdhH50D6Sl9NmawsrWGNWc1csP587nm7JbRX5gvHeznH366jX97bt9ochsRjRgXLKzj8rMauXRpAxe3NZAvFOhOZTk8mKGjP80zu7p54uUuDvQNj+63orWaW165lLdetpihTJ5vrtvDN9e3s793eDSeixYFJ7K1yxtZNa+WxQ2VozWXXL7A1oP9PLO7hx0dgzRUxWmuSdBcXUFDVZxCwcnkC2RyBYayeXZ2pnjpUD/bDgUnvEy+wHjqkjGWt1TTWltBdUWMqkSM6kSUpc1V3HD+fBaPqRUdHszw6KYDbNjTQzYfJPhcIRjmoqYiRk1FLDxOlIIHcWcLTjZfoG8oS08qS89Qhp5UlljEqEoE21YmolTGoyTjUZLxCMl4lAX1SS5cVM+582upTETZ3zvEZx95ke882059ZZzXnj+fLQf62LK/n1zRP1QiGmHV/BpWttaQjIffTzP29Qzx060dLKpP8vGbzuNNFy8at2b49M7D/PXDW/n1zsNHjhmLsLy5mkjE2NudGk0SAIvqk7x6VSvXntNKY3Wcx7d28O+bD7K9YxCA6kSU11+4gDddspCrz26hJ5VlR+fg6OPdVyzlrObqSb+z41HiUOI4roF0jpcO9vPiwX5ePDhAwZ3m6gRN1RU0VScYyubY3jEYPDoHj/lyF6upiNFaWzH6mFdbQU1FjL09Q+zqSrGzc5CuwQyJWIR54frW2grOnV/LhYvruWhxPYvqk8c0P4w0L/xmby/Pt/eSiEV49aoWVi9pmPAX4FRl8wU27uvj/IW1VMSO34TgHiSsXKFA92CW9u4U7d1D7DmcoiIe4eK2Bi5cVEdt8vht2S93DPCzrR001yRYWF/Jwvok8+uSJGKT/y3uzs6uFE/vOMyylmpeuazxmM8rX3CeeLmTgeEcV65opvE4TWMno1Bw+oazdKeydKcy9KQy1FcmWN5STWNV/LTrt9i4r5dP/2gLz7f3ctHiOi5pa2D1kgYWNVTy0qF+tuwPmhB3dg2SzTl5dwoFJxY13nNF0Aw5WTOUu/P0zm6GsnlWtFQflcQB+oaz7O0eIhGLsGKCvp8dnYPsPpziiuVNJOMn3mx4PEocShxHOdQ3zH9s7+KJbV08uaOLXV2p0XXJeISo2TFNAtGIsaSxkhWtNSxprKSlpoKW2gpaaiporknQWhMkgKl8iYezeSpikdPupCIyl0yUONQ5XsbyBWfdzsP0DmW57KxGWsbpnB1I59jZOUjv0MivvSxDmTz1VXFaaoIaQ01FlO0dg2w90M+Wg/1s2tfHjs6gmluXjHHFimbecXkb58yv5dwFtSxprCISMYazebpTGboGMiTjEZY2VU/p1/BUzNYvJBGZfUocZcbdeWZ3D//23D4e+s1+DvUfaVNf3lLN5Wc1Mr+ughcPDrD1QD+7D6eOc7RjLW2q4rwFtdyydgmvWtnC+QvrJuy0TsajYRPKsVfniMjcpcRRYvmCs3l/H7/ecZindx7m6Z3ddA6kScQivOacVt50ySIWNSRZv6ubp3d289jm4JLOFa01XNxWzztfuYSVrdU0hZ2YDZVxKhNRelJZugYzdA2k6R/OsbS5inPm107YSSwiMlU6i5RIOpfn67/ew+cf38bBvqBWsbihklevauGas1t43YXzqSvqXL38rCZuvzaokWTzPmmTUW0yftLXo4uIjEeJ4xTL5Ap8c/0e/uEn29jfO8zaZU184qbzWbu8adwbtsYyMxIxdSiLSOkoccyyTK7Axn29PLO7h2d2d/PrHYfp6E9z6dIG/vrtl3D12c26skhETiuzmjjM7E7gAwQ38n7J3f/ezD4VlnWEm/2puz80zr47gX4gD+RGLgkzsybgG8AyYCfwu+7ePZt/x1TkC87Xfr2bL/9qB33DOdLZPOlcgUy+MHon56L6JGuXN/H2y9t4zTmtShgiclqatcRhZhcRJIi1QAZ42Mx+GK7+O3f/7BQOc527d44puxt4zN0/bWZ3h8sfn6m4T8S6nYf58wc3snFfH5ctbWDt8mYqYsG4PhXxKOctqOWypY0sqE9OfjARkTI3mzWO84En3T0FYGY/A946A8d9C/Ca8PU/AY9TosTRPZjhv/1gE999di8L65P8r1su5Y0XL1RNQkTOaLM5H8cLwLVm1mxmVcDNwJJw3R1m9ryZPWBmjRPs78CPzWy9md1eVD7f3fcDhM/jjt5lZreb2TozW9fR0THeJiflxYP9vPmeX/LD5/fz4etW8thHf4s3XbJISUNEznizljjcfTPwGeBR4GHgOSAHfAFYCawG9gN/M8Ehrnb3y4CbgA+b2bXTfP973X2Nu69pbT1m5sOT8u+bDvLWe37FcLbANz54Jf/1DedRldB1BiIyN8zqDIDufr+7X+bu1wKHgZfc/aC75929AHyJoA9kvH33hc+HgO8WbXfQzBYChM+HZvNvGBMTn398Gx/4yjpWtNbw4B1Xc+nSiSpMIiJnpllNHGY2L3xeCrwN+NrIST/0VoImrbH7VZtZ7chr4PVF2z0I3Ba+vg34/uxEf6zvPruXv3p4K2+8eBHf/NBVGopDROak2W5f+baZNQNZ4MPu3m1mXzGz1QR9GDuBDwKY2SLgPne/GZgPfDfsL4gB/8fdHw6P+WngX83sfcBu4B2z/DeMerljgGjE+Nw7V2v6TxGZs2Y1cbj7q8cpu3WCbfcRdKDj7tuBSybYrgu4YQbDnLL+4Rw1FTElDRGZ02a1qepMMzCcozapTnARmduUOKahL6xxiIjMZUoc0zCQzh41Yq2IyFykxDEN/cM5atRUJSJznBLHNAyk1cchIqLEMQ396uMQEVHimI7gqir1cYjI3KbEMUXpXJ5MvqCmKhGZ85Q4pqh/OAegxCEic54SxxQNhIlDfRwiMtcpcUzRkRqH+jhEZG5T4pii/nQWUI1DRESJY4rUxyEiElDimKIBJQ4REUCJY8r6h4OmKvVxiMhcp8QxRQNpXVUlIgJKHFPWP5yjIhYhEdNHJiJzm86CU9SvAQ5FRAAljinr1zhVIiKAEseUDQxn1b8hIsIUE4eZvdXM6ouWG8zst2ctqjLUr/nGRUSAqdc4/tzde0cW3L0H+PPJdjKzO83sBTPbaGZ3hWWfMrO9ZrYhfNw8zn5LzOynZrY53PfOonWT7j8bBtKai0NEBGCqZ8LxEsxx9zWzi4APAGuBDPCwmf0wXP137v7Z4+yeAz7q7s+YWS2w3swedfdNU9x/xqmPQ0QkMNUaxzoz+1szW2lmK8zs74D1k+xzPvCku6fcPQf8DHjrVN7M3fe7+zPh635gM7B4irHOiv7hrJqqRESYeuL4CEGt4RvAvwJDwIcn2ecF4FozazazKuBmYEm47g4ze97MHjCzxuMdxMyWAZcCTxUVT3n/meDumm9cRCQ0pcTh7oPufre7rwkff+rug5Pssxn4DPAo8DDwHEET1BeAlcBqYD/wNxMdw8xqgG8Dd7l7X1g8pf3N7HYzW2dm6zo6OqbyZ04olclTcN01LiICU7+q6lEzayhabjSzRybbz93vd/fL3P1a4DDwkrsfdPe8uxeALxH0gYz3nnGCpPFVd/9O0TGntL+73zuS6FpbW6fyZ05Ic3GIiBwx1aaqlvBKKgDcvRuYN9lOZjYvfF4KvA34mpktLNrkrQRNWmP3M+B+YLO7/+2YdZPuP9MGRubiUFOViMiUr6oqmNlSd98No/0OPoX9vm1mzUAW+LC7d5vZV8xsdbj/TuCD4TEXAfe5+83A1cCtwG/MbEN4rD9194eAvxpv/9nUpyHVRURGTfVM+GfAL83sZ+HytcDtk+3k7q8ep+zWCbbdR9CBjrv/ErAJtht3/9k0OheH+jhERKaWONz9YTNbQ5AsNgDfJ7iyak5QH4eIyBFTShxm9n7gTqCNIHFcCfwHcP2sRVZG1MchInLEVDvH7wReCexy9+sI7qs4uWtcTyOab1xE5IipJo5hdx8GMLMKd98CnDt7YZWXkcRRnVDiEBGZ6pmwPbyP43vAo2bWDeybraDKTf9wMMBhNDJuf72IyJwy1c7xkTGmPmVmPwXqCe4GnxMG0pqLQ0RkxLTPhu7+s8m3OrNoLg4RkSM0A+AUDKRzuqJKRCSkxDEFfZqLQ0RklBLHFAwMZ3XXuIhISIljCtTHISJyhBLHFGi+cRGRI5Q4JpHLF0hl8urjEBEJKXFMYjCdBzROlYjICCWOSfQNBwMcqo9DRCSgxDGJgbTm4hARKabEMQnNxSEicjQljkloLg4RkaMpcUxCc3GIiBxNiWMS/ZpvXETkKEockxjtHFcfh4gIoMQxqf7hLNGIkYzroxIRgVlOHGZ2p5m9YGYbzeyusOxTZrbXzDaEj5sn2PdGM9tqZtvM7O6i8iYze9TMXgqfG2fzbxgIx6ky0+x/IiIwi4nDzC4CPgCsBS4B3mhmq8LVf+fuq8PHQ+PsGwXuAW4CLgBuMbMLwtV3A4+5+yrgsXB51oxMGysiIoHZrHGcDzzp7il3zwE/A946yT4j1gLb3H27u2eArwNvCde9Bfin8PU/Ab89cyEfqz+tuThERIrNZuJ4AbjWzJrNrAq4GVgSrrvDzJ43swcmaGpaDOwpWm4PywDmu/t+gPB53nhvbma3m9k6M1vX0dFxwn9Ev+biEBE5yqwlDnffDHwGeBR4GHgOyAFfAFYCq4H9wN+Ms/t4HQo+zfe/193XuPua1tbW6ex6lIG05uIQESk2q53j7n6/u1/m7tcCh4GX3P2gu+fdvQB8iaBZaqx2jtROANqAfeHrg2a2ECB8PjR7f0HYx6HEISIyaravqpoXPi8F3gZ8beSkH3orQZPWWE8Dq8xsuZklgHcBD4brHgRuC1/fBnx/NmIfMaDZ/0REjjLbZ8Rvm1kzkAU+7O7dZvYVM1tN0PS0E/gggJktAu5z95vdPWdmdwCPAFHgAXffGB7z08C/mtn7gN3AO2bzDwiuqlLnuIjIiFlNHO7+6nHKbp1g230EHegjyw8Bx1yq6+5dwA0zGOaE0rk8mXxBNQ4RkSK6Hfo4NMChiMixlDiOY0CJQ0TkGEocxzFS41Afh4jIEUocx9Gf1nzjIiJjKXEcx5EahxKHiMgIJY7jGOnjqNNYVSIio5Q4jqN/WPONi4iMpcRxHCOz/6mpSkTkCCWO4+gfzlERi5CI6WMSERmhM+JxaC4OEZFjqQ3mOP7izRfy8TecV+owRETKihLHccSjEeqrVCkTESmms6KIiEyLEoeIiEyLuU9rRtbTkpl1ALtOcPcWoHMGw5kNinHmnA5xKsaZoRgnd5a7HzP39pxIHCfDzNa5+5pSx3E8inHmnA5xKsaZoRhPnJqqRERkWpQ4RERkWpQ4JndvqQOYAsU4c06HOBXjzFCMJ0h9HCIiMi2qcYiIyLQocYiIyLQocRyHmd1oZlvNbJuZ3V3qeADM7AEzO2RmLxSVNZnZo2b2UvjcWOIYl5jZT81ss5ltNLM7yy1OM0ua2a/N7Lkwxr8otxiLYo2a2bNm9oNyjNHMdprZb8xsg5mtK9MYG8zsW2a2JfxeXlWGMZ4bfoYjjz4zu6vc4gQljgmZWRS4B7gJuAC4xcwuKG1UAPwjcOOYsruBx9x9FfBYuFxKOeCj7n4+cCXw4fCzK6c408D17n4JsBq40cyupLxiHHEnsLlouRxjvM7dVxfdc1BuMX4OeNjdzwMuIfg8yypGd98afoargcuBFPBdyixOANxdj3EewFXAI0XLnwA+Ueq4wliWAS8ULW8FFoavFwJbSx3jmHi/D7yuXOMEqoBngCvKLUagjeBkcT3wg3L89wZ2Ai1jysomRqAO2EF4MVA5xjhOzK8HflWucarGMbHFwJ6i5fawrBzNd/f9AOHzvBLHM8rMlgGXAk9RZnGGTUAbgEPAo+5edjECfw98DCgUlZVbjA782MzWm9ntYVk5xbgC6AC+HDb53Wdm1WUW41jvAr4Wvi67OJU4JmbjlOna5Wkwsxrg28Bd7t5X6njGcve8B80CbcBaM7uoxCEdxczeCBxy9/WljmUSV7v7ZQTNuh82s2tLHdAYMeAy4AvufikwSDk090zAzBLAm4FvljqWiShxTKwdWFK03AbsK1EskzloZgsBwudDJY4HM4sTJI2vuvt3wuKyixPA3XuAxwn6jsopxquBN5vZTuDrwPVm9i+UV4y4+77w+RBBm/xayivGdqA9rFECfIsgkZRTjMVuAp5x94PhctnFqcQxsaeBVWa2PPwF8C7gwRLHNJEHgdvC17cR9CmUjJkZcD+w2d3/tmhV2cRpZq1m1hC+rgReC2yhjGJ090+4e5u7LyP4/v3E3d9DGcVoZtVmVjvymqBt/gXKKEZ3PwDsMbNzw6IbgE2UUYxj3MKRZiooxzhL3clSzg/gZuBF4GXgz0odTxjT14D9QJbgl9T7gGaCDtSXwuemEsd4DUGz3vPAhvBxcznFCVwMPBvG+ALwybC8bGIcE+9rONI5XjYxEvQfPBc+No78PymnGMN4VgPrwn/v7wGN5RZjGGcV0AXUF5WVXZwackRERKZFTVUiIjItShwiIjItShwiIjItShwiIjItShwiIjItShwiJRDeR/JUOATGq0/xew+cyveTM0+s1AGIzFE3AFvc/bZJtxQpM6pxyJxkZsvCeRm+FM7H8ePwDnLM7HEzWxO+bgmH/MDM/sDMvmdm/2ZmO8zsDjP7k7DW8KSZNY3zPmeZ2WNm9nz4vNTMVgN/BdwczrtQOWafy83sZ+GggY8UDTfxuJn9vZk9YWYvmNnasLwpjOv5MI6Lw/IaM/tyOFfG82b2O0Xv8T8smIvkSTObPxufsZy5lDhkLlsF3OPuFwI9wO8cf3MALgJ+j2A8pv8BpDwYOO8/gN8fZ/t/AP7Z3S8Gvgr8f+6+Afgk8A0P5l8YGtk4HOPrfwFvd/fLgQfC9xlR7e6vAv44XAfwF8Cz4Xv8KfDPYfn/A/S6+yvCdT8ZOQbwpAdzkfwc+MAU/m6RUWqqkrlsR3gSB1hPMM/JZH7q7v1Av5n1Av8Wlv+GYBiTsa4C3ha+/gpBTeN4ziVITo8GQ34RJRhiZsTXANz952ZWF463dQ1h0nP3n5hZs5nVE4y/9a6RHd29O3yZAX4Qvl5PMFeKyJQpcchcli56nQdGmoxyHKmNJ4+zT6FoucDU/j9NNsaPARvd/aop7u9MPAWATfB+WT8y1lAenQdkmtRUJXKsnQRTdwK8/SSP9QRHfvW/G/jlJNtvBVrN7CoImq7M7MKi9e8My68haIbqJWhuendY/hqg04P5T34M3DGyYznMVS1nBiUOkWN9FvgjM3sCaDnJY/0n4L1m9jxwK8H84RNy9wxBsvqMmT1HMLLwq4o26Q7j+iLByMgAnwLWhO/xaY4Mwf3/Ao1hR/pzwHUn+beIAGh0XJHThZk9DvwXd19X6lhkblONQ0REpkU1DhERmRbVOEREZFqUOEREZFqUOEREZFqUOEREZFqUOEREZFr+f00fQM8XauviAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "\n",
    "axs[0].plot(range(len(loss)),loss)\n",
    "axs[1].plot(range(len(accuracy)),accuracy)\n",
    "\n",
    "plt.setp(axs[1], xlabel='num of epoch')\n",
    "\n",
    "plt.setp(axs[0], ylabel='loss')\n",
    "plt.setp(axs[1], ylabel='acc')\n",
    "\n",
    "plt.savefig('my_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a239a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('my_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b991dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.247 Test accuracy 96.267\n"
     ]
    }
   ],
   "source": [
    "test_loss,accuracy = evaluate_model(model, test_data_iterator, criterion)\n",
    "print(f\"Test Loss {test_loss:.3f} Test accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[25, 13, 14, 29, 13, 13, 13, 25, 13, 13, 25, 21, 13, 13, 29, 13,  2,  2,\n",
    "        13, 13, 13, 25]\n",
    "b=[2, 2, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4affb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb30ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357e06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c5387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n",
    "\n",
    "for batch in train_data_iterator:\n",
    "    x = emb_1(batch.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.permute(0,2,1).reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 1)(x_input)\n",
    "b = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 2)(a)\n",
    "c = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 6)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.squeeze(1).reshape(2,5,-1)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f = {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = nn.Conv1d(in_channels =1 ,out_channels= 4,kernel_size = 3)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"g = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c5cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812f24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ddbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f059bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7f220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e53855",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc472d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943afee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1b13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6634e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act = torch.Tensor([2,0,1])\n",
    "y_pred = torch.argmax(torch.Tensor([[0.1,3,5],[3,2,1],[0,1,0]]),dim=1)\n",
    "y_pred_1 = torch.argmax(torch.Tensor([[1,5,2],[3,2,1],[0,1,0]]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa63f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_1,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b7fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a2799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79f763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f74a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f25d69",
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 1,padding=\"same\")(x_input).squeeze(1)\n",
    "c1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 3,padding=\"same\")(x_input).squeeze(1)\n",
    "d1 = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 5,padding=\"same\")(x_input).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3837d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.cat((b1,c1,d1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13044da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5da08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f020976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a1410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.arange(0,10,1)\n",
    "y1 = np.random.randn(10)\n",
    "y3 = np.random.randn(10)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, sharex='col')\n",
    "ax[0].plot(x,y1)\n",
    "ax[1].plot(x,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9970e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c20c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa5590",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.sqrt(torch.FloatTensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conv 1D layers \n",
    "conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                            out_channels= 1,\n",
    "                                            kernel_size = filter_size,\n",
    "                                            padding = \"same\") \n",
    "                                  for filter_size in [1,3,5]])\n",
    "\n",
    "final_layer = nn.Linear(5*3, 3)\n",
    "\n",
    "# Dropout layers\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "# token_embeddings = [2, 1, 4*5]\n",
    "\n",
    "conved = [F.relu(conv_layer(x_input)).squeeze(1) for conv_layer in conv_layers]\n",
    "print(len(conved))\n",
    "concat = dropout(torch.cat(conved, dim = 1))\n",
    "# [batch size, embedding dimension*num_of_tokens*num_of_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conved = cat.reshape(2,4,-1)\n",
    "# conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "conved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb151724",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = final_layer(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0177c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0b659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e22b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79c6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a1c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d44aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d10c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2212962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn3]",
   "language": "python",
   "name": "conda-env-nn3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "265px",
    "width": "201px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1d36ce9c6112765ad20cbf471b33373de7b9da6b2d9a2b18d6540897511eb08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
