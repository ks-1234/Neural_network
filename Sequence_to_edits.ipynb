{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cba2cf",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4c24",
   "metadata": {},
   "source": [
    "#### seed initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d68987",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "torch.backends.cudnn.determininistic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space\n",
    "3. Build a vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,preprocessing=removeTokens,lower=True,pad_token=\"0\")\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,preprocessing=removeTokens,lower=True,pad_token=\"0\")\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(path='data_filter',train='ptrain.jsonl',test='val.jsonl',\n",
    "                                              format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')\n",
    "edits.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "sort_by_len = lambda x:len(x)\n",
    "\n",
    "train_data_iterator = BucketIterator(train_data,train=True,\n",
    "                                            batch_size=BATCH_SIZE, device= device)#,sort_within_batch=False)\n",
    "\n",
    "test_data_iterator =BucketIterator(test_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e5f1",
   "metadata": {},
   "source": [
    "#### data processing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97f75e0",
   "metadata": {
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7fcf10084a50>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['darling', 'is', 'expected', 'to', 'announce', 'details', 'of', 'tax', 'cuts', 'and', 'plans', 'to', 'increases', 'public'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518a8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = vars(test_data.examples[0])    # to print tokens of any row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7074d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = next(iter(test_data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34c838f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   10, 11375,  5268,  ...,     1,     1,     1],\n",
      "        [ 3091,     0,  4113,  ...,     1,     1,     1],\n",
      "        [  179,   125,   120,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [  539,   270,    25,  ...,     1,     1,     1],\n",
      "        [   28,  3302,    11,  ...,     1,     1,     1],\n",
      "        [    6,     0,   581,  ...,     1,     1,     1]])\n",
      "torch.Size([50, 46])\n"
     ]
    }
   ],
   "source": [
    "print(batch_1.tokens)\n",
    "print(batch_1.edits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef8b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['$$$'] = 0\n",
      "tokens.vocab.itos[0] = <unk>\n",
      "edits.vocab.stoi['$keep'] = 2\n",
      "edits.vocab.itos[2] = $keep\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['$$$'] = {tokens.vocab.stoi['$$$']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[0]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['$keep']}\")\n",
    "print(f\"edits.vocab.itos[2] = {edits.vocab.itos[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab) = 63463\n",
      "len(edits.vocab) = 22\n"
     ]
    }
   ],
   "source": [
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3243d5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab.freqs.keys()) = 63462\n",
      "len(edits.vocab.freqs.keys()) = 20 \n",
      "\n",
      "edits.vocab.freqs = Counter({'$keep': 1051580, '$delete': 39021, '$replace_,': 7424, '$transform_agreement_singular': 6413, '$append_,': 4747, '$append_the': 4689, '$replace_to': 3771, '$replace_the': 3711, '$replace_of': 3585, '$transform_verb_vbz_vb': 3344, '$replace_in': 2996, '$transform_verb_vbg_vb': 2799, '$transform_verb_vbn_vb': 2724, '$append_to': 2575, '$append_of': 2430, '$transform_agreement_plural': 2387, '$append_and': 2314, '$append_a': 2231, '$replace_.': 263, '$append_.': 144})\n"
     ]
    }
   ],
   "source": [
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {edits.vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae29120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.vectors.shape = torch.Size([63463, 100])\n",
      "edits.vocab.vectors.shape = torch.Size([22, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens.vocab.vectors.shape}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits.vocab.vectors.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7484bbd",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9881b4ee",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        self.position_embedding_layer = nn.Embedding(max_lenght,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\") \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        \n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        position = torch.arange(0,num_of_tokens).unsqueeze(0)         # [1,num_of_tokens]\n",
    "        position = position.repeat(batch_size,1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.token_embedding_layer(tokens)\n",
    "        # token_embeddings = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        # elementwise sum of tokens and position \n",
    "        combination = self.dropout(token_embeddings)\n",
    "        # combination = [batch_size, num_of_tokens,embedding dimension]\n",
    "        #print(f\"batch_size, num_of_tokens,embedding dimension = {combination.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conved = combination.permute(0, 2, 1).reshape(batch_size,-1).unsqueeze(1)\n",
    "        #conv_input = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        #print(f\"conv lay input = {conved.shape}\")\n",
    "        \n",
    "        # multiple convolution and activation layers\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv_layer(self.dropout(conved))\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "            \n",
    "            #pass through relu activation function\n",
    "            conved = F.relu(conved)*self.scale\n",
    "\n",
    "            #conved = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        \n",
    "        #print(f\"conv lay output = {conved.shape}\")\n",
    "        \n",
    "        conved = conved.squeeze(1).reshape(batch_size,num_of_tokens,-1)\n",
    "        #conved = [batch_size, num_of_tokens, embedding_dimension]\n",
    "        \n",
    "        \n",
    "        output = self.final_layer(conved+combination)*self.scale\n",
    "        #output = [batch size, num_of_tokens, target dimension]\n",
    "        #print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad412d2",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07bcb53",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "clip = 0.1\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 3\n",
    "FILTER_SIZES = [1,2,4]\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074921ca",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c38ca13d",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq_to_edits(INPUT_DIM,OUTPUT_DIM,OUTPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,DROPOUT,PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fa205d5",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,358,532 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862f3c77",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.6149,  0.9273,  0.5583,  ..., -0.0922, -0.0798, -1.2843],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-tained embeddings \n",
    "\n",
    "pretrained_embeddings = tokens.vocab.vectors\n",
    "\n",
    "model.token_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c21b7ae",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "\n",
    "model.token_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.token_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d081c6",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "n_total_steps = len(train_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f7907ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq_to_edits(\n",
       "  (token_embedding_layer): Embedding(63463, 100)\n",
       "  (position_embedding_layer): Embedding(100, 100)\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(1, 1, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(1, 1, kernel_size=(2,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=100, out_features=22, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0798e",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f71b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,data_iterator,optimizer,criterion,clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        \n",
    "        # Make the gradient vector to zero so \n",
    "        #So not to added with the new gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        # get output from the model\n",
    "        outputs = model(tokens_list)\n",
    "        # outputs = [batch size, num_of_tokens, embedding dimension]\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "        # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "        trg = batch.edits.contiguous().view(-1)\n",
    "        # trg = [batch size*num_of_tokens]\n",
    "        \n",
    "        loss = criterion(outputs, trg)\n",
    "\n",
    "        # Backward and optimize\n",
    "        \n",
    "        # to calculate gradient\n",
    "        loss.backward()\n",
    "        #to avoid exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # to make the updates in the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return epoch_loss / len(data_iterator)\n",
    "\n",
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "921e2892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            # converting to cpu or gpu variable\n",
    "            tokens_list = batch.tokens.to(device)\n",
    "            edits_list = batch.edits.to(device)\n",
    "            \n",
    "            outputs = model(tokens_list)\n",
    "            # outputs = [batch size, num_of_tokens, embedding dimension]\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "            # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "            trg = batch.edits.contiguous().view(-1)\n",
    "            # trg = [batch size*num_of_tokens]\n",
    "            loss = criterion(outputs, trg)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            print(f\"pre = {predicted}\")\n",
    "            print(f\"edit = {edits_list}\")\n",
    "            epoch_loss += loss.item()\n",
    "            n_samples += edits_list.size(0)\n",
    "            n_correct += (predicted == edits_list).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return epoch_loss / len(data_iterator),acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddc3b",
   "metadata": {},
   "source": [
    "#### actual train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223181f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirthika-13378/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.391\n",
      "Epoch [2/50], Train Loss: 0.192\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model,train_data_iterator,optimizer,criterion,clip)\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}')\n",
    "except KeyboardInterrupt:\n",
    "    end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356eb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate_model(model, test_data_iterator, criterion)\n",
    "print(f\"Test Loss {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cf21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fe2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6612c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30019406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54443b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1b254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154c1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c5387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n",
    "\n",
    "for batch in train_data_iterator:\n",
    "    x = emb_1(batch.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = 40\n",
    "C_in = 40\n",
    "L_in = 100\n",
    "inputs = torch.rand([N, C_in, L_in])\n",
    "padding = 3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "C_out = 10\n",
    "x = torch.nn.Conv1d(C_in, C_out, kernel_size, stride=stride, padding=padding)\n",
    "y = x(inputs)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.permute(0,2,1).reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 1,padding=\"same\")(x_input)\n",
    "b = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 1,padding=\"same\")(a)\n",
    "c = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 3,padding=\"same\")(b)\n",
    "d = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 4,padding=\"same\")(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")\n",
    "print(f\"d = {d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.squeeze(1).reshape(2,5,-1)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f = {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = nn.Conv1d(in_channels =1 ,out_channels= 4,kernel_size = 3)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"g = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f25d69",
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a1 = {a1.shape}\")\n",
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5da08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn3]",
   "language": "python",
   "name": "conda-env-nn3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "135px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1d36ce9c6112765ad20cbf471b33373de7b9da6b2d9a2b18d6540897511eb08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
