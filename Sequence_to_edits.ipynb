{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "\n",
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import precision_recall,f1_score\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from seqeval.metrics import accuracy_score\n",
    "import seqeval.metrics\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06f1da",
   "metadata": {},
   "source": [
    "##### instal packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d488e8f",
   "metadata": {},
   "source": [
    "conda install pytorch=1.9 torchvision torchaudio -c pytorch\n",
    "conda install -y torchtext==0.10 -c pytorch\n",
    "conda install -c conda-forge torchmetrics\n",
    "pip install seqeval\n",
    "conda install -c conda-forge matplotlib-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cba2cf",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #1,2,4,8,16,32,64,128,256,512,1028\n",
    "path = \"data_filter\"\n",
    "# path = \"data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4c24",
   "metadata": {},
   "source": [
    "#### seed initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "11c135d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "torch.backends.cudnn.determininistic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space\n",
    "3. Build a vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "# removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"<pad>\")\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"<pad>\")\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(path=path,train='ptrain.jsonl',test='val.jsonl',\n",
    "                                              format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "10798651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52731"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')\n",
    "edits.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')\n",
    "\n",
    "# os =  RandomOverSampler()\n",
    "# X_train_res, y_train_res = os.fit_sample(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4bbe23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', \n",
    " 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', \n",
    " 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "train_data_iterator = BucketIterator(train_data,train=True,\n",
    "                                            batch_size=BATCH_SIZE, device= device,shuffle=True)#,sort_within_batch=False)\n",
    "\n",
    "test_data_iterator =BucketIterator(test_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e5f1",
   "metadata": {},
   "source": [
    "#### data processing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "92295815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52731"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e97f75e0",
   "metadata": {
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7fcc62dc73d0>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['alistair', 'darling', 'is', 'expected', 'to', 'announce', 'details', 'of', 'tax', 'cuts', 'and', 'plans', 'to', 'increases', 'public', 'spending'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$append_.']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "518a8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = vars(test_data.examples[0])    # to print tokens of any row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7074d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = next(iter(train_data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3ef8b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['0'] = 0\n",
      "tokens.vocab.itos[0] = <unk>\n",
      "edits.vocab.stoi['$keep'] = 0\n",
      "edits.vocab.itos[1] = <pad>\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['0'] = {tokens.vocab.stoi['<unk>']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[0]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['0']}\")\n",
    "print(f\"edits.vocab.itos[1] = {edits.vocab.itos[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "547693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab) = 65708\n",
      "len(edits.vocab) = 22\n"
     ]
    }
   ],
   "source": [
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8d3243d5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab.freqs.keys()) = 65706\n",
      "len(edits.vocab.freqs.keys()) = 20 \n",
      "\n",
      "edits.vocab.freqs = 0\n"
     ]
    }
   ],
   "source": [
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {tokens.vocab.freqs['<pad>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ae29120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.vectors.shape = torch.Size([65708, 100])\n",
      "edits.vocab.vectors.shape = torch.Size([22, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens.vocab.vectors.shape}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits.vocab.vectors.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7484bbd",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9881b4ee",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 200):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.channels = 1\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        self.position_embedding_layer = nn.Embedding(max_lenght,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        for filter_size in filter_sizes:\n",
    "            self.conv_layers.append(nn.Conv1d(in_channels = self.channels,out_channels= 10,\n",
    "                                         kernel_size = filter_size,padding = \"same\"))\n",
    "            self.channels = 10\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv1d(in_channels = self.channels,out_channels= 1,\n",
    "                                         kernel_size = 1,padding = \"same\"))\n",
    "        \n",
    "        # Linear layer\n",
    "        self.final_layer = nn.Linear(embed_dim*10, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        \n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        position = torch.arange(0,num_of_tokens).unsqueeze(0)         # [1,num_of_tokens]\n",
    "        position = position.repeat(batch_size,1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.token_embedding_layer(tokens)\n",
    "        position_embeddings = self.position_embedding_layer(position)\n",
    "        # token_embeddings = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        # elementwise sum of tokens and position \n",
    "        combination = self.dropout(token_embeddings+position_embeddings)\n",
    "        # combination = [batch_size, num_of_tokens,embedding dimension]\n",
    "        #print(f\"batch_size, num_of_tokens,embedding dimension = {combination.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conv_input = combination.reshape(batch_size,-1).unsqueeze(1)\n",
    "        #conv_input = [batch size, 1, embedding dimension*num_of_tokens]\n",
    "        #print(f\"conv lay input = {conved.shape}\")\n",
    "        \n",
    "        # multiple convolution and activation layers\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv_layer(self.dropout(conv_input))\n",
    "            #conved = [batch size, 1, embedding_dimension*num_of_tokens]\n",
    "            \n",
    "            #pass through relu activation function\n",
    "            conved = F.relu(conved)\n",
    "\n",
    "            #conved = [batch size, 1, embedding_dimension*num_of_tokens]\n",
    "            \n",
    "            conved = (conved+conv_input)*self.scale\n",
    "            \n",
    "            conv_input = conved\n",
    "        \n",
    "        conved = conved.squeeze(1).reshape(batch_size,num_of_tokens,-1)\n",
    "        # conved = [batch_size, num_of_tokens, embedding_dimension]\n",
    "        # print(f\"conv laycombinationutput = {conved.shape}\")\n",
    "        \n",
    "        output = self.final_layer(conved)*self.scale\n",
    "        #output = [batch size, num_of_tokens, target dimension]\n",
    "        #print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "995e0285",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list              # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list                   # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int                     # embedding dimension\n",
    "               ,num_of_filters : int                # to num of filters\n",
    "               ,filter_sizes : list                 # list of sizes of each filter\n",
    "               ,dropout : int                       # dropout percent ()\n",
    "               ,pad_index : int                     # padding index from the vocab\n",
    "               ,max_lenght : int = 100):            # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(token_vocab_size,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                    out_channels= 1,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    padding = \"same\",device=self.device) \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(num_of_filters*embed_dim, edit_vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "     def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.dropout(self.token_embedding_layer(tokens))\n",
    "        # token_embeddings = [batch_size, num_of_tokens, embedding dimension]\n",
    "        # print(f\"batch_size, num_of_tokens,embedding_dimension = {token_embeddings.shape}\")\n",
    "\n",
    "        # Apply perute similar to transposing in multi dimension\n",
    "        conv_input = token_embeddings.reshape(batch_size,-1).unsqueeze(1)\n",
    "        # conv_input = [batch size, 1, embedding_dimension*num_of_tokens]\n",
    "        # print(f\"conv lay input = {conv_input.shape}\")\n",
    "        \n",
    "        conved = [conv_layer(conv_input).squeeze(1) for conv_layer in self.conv_layers]\n",
    "        \n",
    "        cat = self.dropout(F.relu(torch.cat(conved, dim = 1)))\n",
    "        # [batch size, embedding dimension*num_of_tokens*num_of_filter]\n",
    "\n",
    "        conved = cat.reshape(batch_size,num_of_tokens,-1)\n",
    "        # conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "        # print(f\"conved before output = {conved.shape}\")\n",
    "        \n",
    "        output = self.final_layer(conved)*self.scale\n",
    "        # output = [batch size, num_of_tokens, target dimension]\n",
    "        # print(f\"output = {output.shape}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0798e",
   "metadata": {},
   "source": [
    "\n",
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b1134a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(vocab_freq):\n",
    "    total_freq = sum(vocab_freq.values())\n",
    "    \n",
    "    class_weights =[1-(counts/total_freq) for edit,counts in vocab_freq.items()]\n",
    "    class_weights = torch.tensor(class_weights).to(device)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "80f71b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,data_iterator,optimizer,criterion,clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss,acc,f1_point = 0,0,0\n",
    "        \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        \n",
    "        # Make the gradient vector to zero so \n",
    "        #So not to added with the new gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        # get output from the model\n",
    "        outputs = model(tokens_list)\n",
    "        # outputs = [batch size, num_of_tokens, target dimension]\n",
    "        \n",
    "        outputs = outputs.contiguous().view(-1, OUTPUT_DIM)\n",
    "        # outputs = [batch size, num_of_tokens*embedding dimension]\n",
    "        \n",
    "        trg = batch.edits.contiguous().view(-1)\n",
    "        # trg = [batch size*num_of_tokens]\n",
    "        # print(f\"out dim = {outputs_1.shape}\")\n",
    "        \n",
    "        loss = criterion(outputs, trg)\n",
    "\n",
    "        # Backward and optimize\n",
    "        \n",
    "        # to calculate gradient\n",
    "        loss.backward()\n",
    "        #to avoid exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # to make the updates in the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        predicted = torch.argmax(outputs, dim=-1)\n",
    "        # print(f\"pre = {predicted.shape}\")\n",
    "        # print(f\"edit = {trg.shape}\")\n",
    "        \n",
    "        acc += accuracy_score(predicted, trg).item()\n",
    "        # TP+TN / TP+TN+FP+FN\n",
    "        f1_point += f1_score(predicted, trg).item()\n",
    "        # f1_score = (precission*recall)/(precission+recall)\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    f1_point = f1_point / len(data_iterator)\n",
    "    \n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc,f1_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "921e2892",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss,acc,f1_point = 0,0,0\n",
    "    \n",
    "    testing = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            \n",
    "            # converting to cpu or gpu variable\n",
    "            tokens_list = batch.tokens.to(device)\n",
    "            edits_list = batch.edits.to(device)\n",
    "            \n",
    "            outputs = model(tokens_list)\n",
    "            # outputs = [batch size, num_of_tokens, target_dimension]\n",
    "            \n",
    "            bact_size,num_tokens,output_dim = outputs.shape\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, output_dim)\n",
    "            # outputs = [batch size*num_of_tokens,embedding dimension]\n",
    "            trg = edits_list.contiguous().view(-1)\n",
    "            # trg = [batch size*num_of_tokens]\n",
    "            loss = criterion(outputs, trg)\n",
    "            epoch_loss += loss.item() \n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "            # print(f\"pre = {predicted.shape}\")\n",
    "            # print(f\"edit = {edits_list.shape}\")\n",
    "            acc += accuracy_score(predicted, trg).item()\n",
    "            f1_point += f1_score(predicted, trg,average='macro',mdmc_average='global',num_classes=22).item()\n",
    "#             f1_score()\n",
    "            # f1_score = (precission*recall)/(precission+recall)\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    f1_point = f1_point / len(data_iterator)\n",
    "\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss / len(data_iterator),acc,f1_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "82fbcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_edit(token,token_vocab,edit_vocab,model,device):\n",
    "    model.eval()\n",
    "    \n",
    "    token = [token_vocab.vocab.stoi[word] for word in token]\n",
    "    token_tensor=torch.tensor(token,dtype=torch.long).unsqueeze(0).to(device)\n",
    "    # token_tensor = [1, token_len]\n",
    "    with torch.no_grad():\n",
    "        output = model(token_tensor).squeeze(0)\n",
    "        pred_token = torch.argmax(output,-1)\n",
    "        pred_token = [edit_vocab.vocab.itos[word] for word in pred_token]\n",
    "    return pred_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b1083aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(batch_data,num_classes):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        tokens_list = batch_data.tokens.to(device)\n",
    "        edits_list = batch_data.edits.to(device)\n",
    "        \n",
    "        outputs = torch.argmax(model(tokens_list),dim=-1)\n",
    "    \n",
    "    return f1_score(outputs,edits_list,average=None,num_classes=num_classes,mdmc_average=\"samplewise\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "da9430d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_positive(data_iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    testing = torch.zeros(22)\n",
    "    total = torch.zeros(22)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            \n",
    "            tokens_list = batch.tokens.to(device)\n",
    "            edits_list = batch.edits.to(device)\n",
    "            \n",
    "            outputs = model(tokens_list)\n",
    "            \n",
    "            bact_size,num_tokens,output_dim = outputs.shape\n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=-1).reshape(bact_size,num_tokens)\n",
    "            actual = edits_list\n",
    "            \n",
    "            for result in range(len(actual)):\n",
    "                for p,a in zip(predicted,actual):\n",
    "                    total[actual] += 1\n",
    "                    if p==a:\n",
    "                        testing[actual] += 1\n",
    "\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (total, testing)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8911b3c9",
   "metadata": {},
   "source": [
    "XXX--------Not for use--------XXX\n",
    "\n",
    "def calculate_accuracy(data_iterator, token_vocab, edit_vocab, model, device):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for data in data_iterator:\n",
    "        \n",
    "        src = data.tokens\n",
    "        trg = data.edits\n",
    "        \n",
    "        pred_trg = token_edit(src, token_vocab, edit_vocab, model, device)\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append(trg)\n",
    "        \n",
    "    return accuracy_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad412d2",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b07bcb53",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "clip = 0.1\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "EMBEDDING_DIM = tokens.vocab.vectors.shape[1]   #vocabular size, dim\n",
    "FILTER_SIZES = [1,3,5]\n",
    "N_FILTERS = len(FILTER_SIZES)\n",
    "weight_decay = 0.001\n",
    "amsgrad = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074921ca",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c38ca13d",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq_to_edits(INPUT_DIM,OUTPUT_DIM,OUTPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,DROPOUT,PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bbec79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_edits = sum(edits.vocab.freqs.values())\n",
    "class_weights = torch.Tensor([1-value/total_edits for key,value in edits.vocab.freqs.items()])\n",
    "class_weights = torch.cat((torch.Tensor([1-1/total_edits,1-1/total_edits]),class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "40d081c6",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9fa205d5",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,613,673 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "862f3c77",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-tained embeddings \n",
    "\n",
    "pretrained_embeddings = tokens.vocab.vectors\n",
    "# model = torch.load(path+'/model.pt')\n",
    "model.token_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6c21b7ae",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "model.token_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.token_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9f7907ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq_to_edits(\n",
       "  (token_embedding_layer): Embedding(65708, 100)\n",
       "  (position_embedding_layer): Embedding(200, 100)\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(1, 10, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(10, 10, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (3): Conv1d(10, 1, kernel_size=(1,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=1000, out_features=22, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddc3b",
   "metadata": {},
   "source": [
    "### Train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "82b0f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2223181f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.566, Train Accuracy: 72.927, F1 score: 0.729\n",
      "Epoch [2/100], Train Loss: 0.553, Train Accuracy: 73.762, F1 score: 0.738\n",
      "Epoch [3/100], Train Loss: 0.551, Train Accuracy: 73.852, F1 score: 0.739\n",
      "Epoch [4/100], Train Loss: 0.550, Train Accuracy: 73.843, F1 score: 0.738\n",
      "Epoch [5/100], Train Loss: 0.553, Train Accuracy: 73.801, F1 score: 0.738\n",
      "Epoch [6/100], Train Loss: 0.552, Train Accuracy: 73.922, F1 score: 0.739\n",
      "Epoch [7/100], Train Loss: 0.552, Train Accuracy: 73.755, F1 score: 0.738\n",
      "Epoch [8/100], Train Loss: 0.551, Train Accuracy: 73.946, F1 score: 0.739\n",
      "Epoch [9/100], Train Loss: 0.549, Train Accuracy: 73.819, F1 score: 0.738\n",
      "Epoch [10/100], Train Loss: 0.552, Train Accuracy: 73.767, F1 score: 0.738\n",
      "Epoch [11/100], Train Loss: 0.551, Train Accuracy: 74.038, F1 score: 0.740\n",
      "Epoch [12/100], Train Loss: 0.550, Train Accuracy: 74.038, F1 score: 0.740\n",
      "Epoch [13/100], Train Loss: 0.551, Train Accuracy: 73.922, F1 score: 0.739\n",
      "Epoch [14/100], Train Loss: 0.552, Train Accuracy: 73.900, F1 score: 0.739\n",
      "Epoch [15/100], Train Loss: 0.548, Train Accuracy: 74.004, F1 score: 0.740\n",
      "Epoch [16/100], Train Loss: 0.551, Train Accuracy: 73.915, F1 score: 0.739\n",
      "Epoch [17/100], Train Loss: 0.554, Train Accuracy: 73.812, F1 score: 0.738\n",
      "Epoch [18/100], Train Loss: 0.550, Train Accuracy: 74.048, F1 score: 0.740\n",
      "Epoch [19/100], Train Loss: 0.550, Train Accuracy: 73.907, F1 score: 0.739\n",
      "Epoch [20/100], Train Loss: 0.548, Train Accuracy: 74.032, F1 score: 0.740\n",
      "Epoch [21/100], Train Loss: 0.550, Train Accuracy: 73.971, F1 score: 0.740\n",
      "Epoch [22/100], Train Loss: 0.549, Train Accuracy: 73.894, F1 score: 0.739\n",
      "Epoch [23/100], Train Loss: 0.549, Train Accuracy: 74.012, F1 score: 0.740\n",
      "Epoch [24/100], Train Loss: 0.546, Train Accuracy: 74.208, F1 score: 0.742\n",
      "Epoch [25/100], Train Loss: 0.549, Train Accuracy: 73.881, F1 score: 0.739\n",
      "Epoch [26/100], Train Loss: 0.549, Train Accuracy: 73.890, F1 score: 0.739\n",
      "Epoch [27/100], Train Loss: 0.549, Train Accuracy: 74.037, F1 score: 0.740\n",
      "time take is 106.246 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_6923/1845964056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}, F1 score: {f1_sc:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_6923/2217781973.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# get output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# outputs = [batch size, num_of_tokens, target dimension]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_6923/1889117246.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#pass through convolutional layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mconved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;31m#conved = [batch size, 1, embedding_dimension*num_of_tokens]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    294\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 295\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.perf_counter_ns()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss,train_acc,f1_sc = train_model(model,train_data_iterator,optimizer,criterion,clip)\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}, F1 score: {f1_sc:.3f}')\n",
    "        loss.append(train_loss)\n",
    "        accuracy.append(train_acc)\n",
    "    \n",
    "finally:\n",
    "    end = time.perf_counter_ns()\n",
    "    print(f\"time take is {(end-start)*10**-9*0.0166667:.3f} min\")\n",
    "    torch.save(model, path+'/model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0fc3ea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(loss)),loss)\n",
    "axs[1].plot(range(len(accuracy)),accuracy)\n",
    "\n",
    "plt.setp(axs[1], xlabel='num of epoch')\n",
    "\n",
    "plt.setp(axs[0], ylabel='loss')\n",
    "plt.setp(axs[1], ylabel='acc')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b991dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.670 Test accuracy 83.085 F1 score 0.096\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy,f1 = evaluate_model(model, test_data_iterator, criterion)\n",
    "print(f\"Test Loss {test_loss:.3f} Test accuracy {test_accuracy:.3f} F1 score {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "32f09faa",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.8061, 0.8259, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "pre_recall = calculate_f1_score(next(iter(test_data_iterator)),len(edits.vocab))\n",
    "print(pre_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d700cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[25, 13, 14, 29, 13, 13, 13, 25, 13, 13, 25, 21, 13, 13, 29, 13,  2,  2,\n",
    "        13, 13, 13, 25]\n",
    "b=[2, 2, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e32a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70a859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185d75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dce7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4c31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n",
    "\n",
    "for batch in train_data_iterator:\n",
    "    x = emb_1(batch.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.permute(0,2,1).reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 1)(x_input)\n",
    "b = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 2)(a)\n",
    "c = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 6)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.squeeze(1).reshape(2,5,-1)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f = {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = nn.Conv1d(in_channels =1 ,out_channels= 4,kernel_size = 3)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"g = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcc818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f9673",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48fbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9f9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464793ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act = torch.Tensor([[[2,0,1],[2,0,1]],[[2,0,1],[2,0,1]]]).int()\n",
    "# batch size, tokens, emb lay\n",
    "# 2, 3, 3\n",
    "y_pred = torch.argmax(torch.Tensor([[[[0,2,1],[4,2,1],[0,1,0]],[[0,0,1],[3,2,1],[0,1,0]]],[[[1,5,2],[3,2,1],[0,1,0]],[[1,5,2],[3,2,1],[0,1,0]]]]),dim=-1)\n",
    "y_pred_1 = torch.argmax(torch.Tensor([[[1,5,2],[3,2,1],[0,1,0]],[[1,5,2],[3,2,1],[0,1,0]]]).reshape(-1,3),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(y_pred,y_act,average=None,num_classes=3,mdmc_average='samplewise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68931167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_1,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa505f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e81a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b626a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f25d69",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b1 = nn.Conv1d(in_channels =1 ,out_channels= 2,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =2 ,out_channels= 6,kernel_size = 3,padding=\"same\")(b1)\n",
    "d1 = nn.Conv1d(in_channels =6 ,out_channels= 1,kernel_size = 5,padding=\"same\")(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.cat((b1,c1,d1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6a696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5da08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9cd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.arange(0,10,1)\n",
    "y1 = np.random.randn(10)\n",
    "y3 = np.random.randn(10)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, sharex='col')\n",
    "ax[0].plot(x,y1)\n",
    "ax[1].plot(x,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8b8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9279fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3bea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde2da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35753d30",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.sqrt(torch.FloatTensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conv 1D layers \n",
    "conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                            out_channels= 1,\n",
    "                                            kernel_size = filter_size,\n",
    "                                            padding = \"same\") \n",
    "                                  for filter_size in [1,3,5]])\n",
    "\n",
    "final_layer = nn.Linear(5*3, 3)\n",
    "\n",
    "# Dropout layers\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "# token_embeddings = [2, 1, 4*5]\n",
    "conved = x_input\n",
    "for conv_layer in conv_layers:\n",
    "    conved = F.relu(conv_layer(conved))\n",
    "print(len(conved))\n",
    "concat = dropout(torch.cat(conved, dim = 1))\n",
    "# [batch size, embedding dimension*num_of_tokens*num_of_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conved = concat.reshape(2,4,-1)\n",
    "# conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "conved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = final_layer(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a914c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd4dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e8b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"tokens\": [\"Alistair\", \"Darling\", \"is\", \"expected\", \"to\", \"announce\", \"details\", \"of\", \n",
    "                \"tax\", \"cuts\", \"and\", \"plans\", \"to\", \"increases\", \"public\", \"spending\"], \n",
    "     \"labels\": [\"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \n",
    "                \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$TRANSFORM_VERB_VBZ_VB\", \"$KEEP\", \"$APPEND_.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ed987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_accuracy(test_data,tokens,edits,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$TRANSFORM_VERB_VBZ_VB\", \"$KEEP\", \"$APPEND_.\"] \n",
    "b = ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$keep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(a,b):\n",
    "    print(i.lower()==j,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17b890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371f769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = torch.Tensor([[[1,2,10],[3,4,11]],[[5,6,12],[7,8,13]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5e895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = [0] * len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658daf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "filterss = [2,3,4]\n",
    "inf = 1\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "# print(x_input)\n",
    "\n",
    "conv_layers = nn.ModuleList()\n",
    "\n",
    "for i,filter_size in enumerate(filterss):\n",
    "    conv_layers.append(nn.Conv1d(in_channels = inf,out_channels= 10,\n",
    "                                 kernel_size = filter_size,padding = \"same\"))\n",
    "    inf = 10\n",
    "\n",
    "con = x_input\n",
    "print(\"in\",con.shape)\n",
    "\n",
    "for i, conv_layer in enumerate(conv_layers):\n",
    "    #pass through convolutional layer\n",
    "    print(f\"{i} input {con.shape}\")\n",
    "    \n",
    "    conved = F.relu(conv_layer(con))\n",
    "    print(f\"{i} iiiii {conved.shape}\")\n",
    "    conved = conved + con\n",
    "    con = conved\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e81c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732268a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deb74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144754b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3415ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e5dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad7ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce485b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afe7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46f399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd35a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40550a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ced2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data,label) in enumerate(train_data_iterator):\n",
    "    print(label)\n",
    "    break\n",
    "class_weights= torch.Tensor([1/value for key,value in edits.vocab.freqs.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [batch.edits for batch in train_data_iterator]\n",
    "l1 = [lst for edit in l1 for lst in edit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5393e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fc251",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "for data in train_data_iterator:\n",
    "        if len(files) > 0:\n",
    "            class_weights.append(1/len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (token, edit) in enumerate(train_data):\n",
    "    class_weight = sum([class_weight for class_weight in eidts])\n",
    "    sample_weights[idx] = class_weight\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d972781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e791ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde83cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549513e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc6dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c91dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eb9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd164b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c186c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598388d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b6770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165ba90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb22d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aeae6e2",
   "metadata": {},
   "source": [
    "###  THE END"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn1]",
   "language": "python",
   "name": "conda-env-nn1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "316px",
    "width": "201px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "685px",
    "left": "26px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1d36ce9c6112765ad20cbf471b33373de7b9da6b2d9a2b18d6540897511eb08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
