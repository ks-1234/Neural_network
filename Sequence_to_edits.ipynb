{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b792ac7",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Build a vocab.\n",
    "3. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,preprocessing=removeTokens,lower=True,pad_token=\"0\")\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,preprocessing=removeTokens,lower=True,pad_token=\"0\")\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(path='data',train='ptrain.jsonl',test='val.jsonl',\n",
    "                                              format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e97f75e0",
   "metadata": {
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7fa4dabe1690>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0ad9a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$keep',\n",
       " '$transform_verb_vbz_vb',\n",
       " '$keep',\n",
       " '$append_.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[1].edits    # to print tokens of any row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')\n",
    "edits.build_vocab(train_data,min_freq=1,vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9c0037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.field.Field object at 0x7fa4dabe1490>\n",
      "<torchtext.legacy.vocab.Vocab object at 0x7fa4da203d50>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokens)   # field object\n",
    "print(tokens.vocab)   #vocab object\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())\n",
    "print(len(train_data[0].__dict__.values()))\n",
    "\n",
    "# Note: even after the building vocab the train data will not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "train_data_iterator, test_data_iterator = BucketIterator.splits((train_data,test_data),\n",
    "                                                                batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7074d5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, 22,  2,  7])\n"
     ]
    }
   ],
   "source": [
    "batch_1 = iter(train_data_iterator)\n",
    "batch_1 = next(batch_1)\n",
    "print(batch_1.tokens.shape)\n",
    "print(batch_1.edits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3243d5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['$$$'] = 0\n",
      "tokens.vocab.itos[0] = <unk>\n",
      "edits.vocab.stoi['$keep'] = 2\n",
      "edits.vocab.itos[2] = $keep\n",
      "\n",
      "len(tokens.vocab) = 186\n",
      "len(edits.vocab) = 23\n",
      "\n",
      "tokens.vocab.vectors.shape = torch.Size([186, 100])\n",
      "edits.vocab.vectors.shape = torch.Size([23, 100])\n",
      "\n",
      "len(tokens.vocab.freqs.keys()) = 184\n",
      "len(edits.vocab.freqs.keys()) = 21 \n",
      "\n",
      "edits.vocab.freqs = Counter({'$keep': 270, '$delete': 11, '$replace_.': 5, '$transform_agreement_singular': 2, '$append_every': 2, '$replace_might': 1, '$replace_,': 1, '$transform_verb_vbz_vb': 1, '$append_.': 1, '$transform_case_lower': 1, '$append_had': 1, '$replace_the': 1, '$append_said': 1, '$replace_no': 1, '$transform_verb_vbn_vbg': 1, '$replace_by': 1, '$append_yesterday': 1, '$replace_are': 1, '$replace_so': 1, '$append_for': 1, '$replace_what': 1})\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['$$$'] = {tokens.vocab.stoi['$$$']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[0]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['$keep']}\")\n",
    "print(f\"edits.vocab.itos[2] = {edits.vocab.itos[2]}\")\n",
    "print()\n",
    "\n",
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")\n",
    "print()\n",
    "\n",
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens.vocab.vectors.shape}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits.vocab.vectors.shape}\")\n",
    "print()\n",
    "\n",
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {edits.vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bb70b",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9881b4ee",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq_to_edits(nn.Module):\n",
    "    def __init__(self,token_vocab_size : int        # num of unique token in the entire dataset \n",
    "               ,edit_vocab_size : list             # [batch_size,max sentence size in the batch]\n",
    "               ,output_dim : list            # [batch_size,max sentence size in the batch]\n",
    "               ,embed_dim : int              # embedding dimension\n",
    "               ,num_of_filters : int         # to num of filters\n",
    "               ,filter_sizes : list          # list of sizes of each filter\n",
    "               ,dropout : int                # dropout percent ()\n",
    "               ,pad_index : int              # padding index from the vocab\n",
    "               ,max_lenght : int = 100):     # Maximum words in a list of tokens or sentence\n",
    "               \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.position_embedding_layer = nn.Embedding(max_lenght,embed_dim)\n",
    "        \n",
    "        # Multiple conv 1D layers \n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(in_channels = embed_dim,\n",
    "                                                    out_channels= embed_dim,\n",
    "                                                    kernel_size = filter_size,\n",
    "                                                    stride = filter_size) \n",
    "                                          for filter_size in filter_sizes])\n",
    "        \n",
    "        #self.final_layer = nn.Conv1d(in_channels = embed_dim*len(filter_sizes),out_channels=output_dim,kernel_size = 1)\n",
    "        \n",
    "        # Dropout layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        # tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "        \n",
    "        batch_size , num_of_tokens = tokens.shape\n",
    "        \n",
    "        position = torch.arange(0,num_of_tokens).unsqueeze(0)         # [1,num_of_tokens]\n",
    "        position = position.repeat(batch_size,1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        \n",
    "        # apply embedding layer\n",
    "        token_embeddings    = self.token_embedding_layer(tokens)\n",
    "        position_embeddings = self.position_embedding_layer(position)\n",
    "        # token_embeddings = position_embeddings = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        combination = self.dropout(token_embeddings+position_embeddings)\n",
    "        # combination = [batch_size, num_of_tokens,embedding dimension]\n",
    "        \n",
    "        conv_input = combination.reshape(batch_size,-1).unsqueeze(1)\n",
    "        # conv_input = [batch_size, embedding dimension, num_of_tokens]\n",
    "        \n",
    "        \n",
    "        conved = [F.relu(conv_layer(conv_input)).squeeze(1) for conv_layer in self.conv_layers]\n",
    "        # conved = [batch_size, embedding dimension, num_of_tokens]\n",
    "        \n",
    "        output = torch.cat(conved,dim=1)\n",
    "        \n",
    "        return output \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895062d",
   "metadata": {},
   "source": [
    "### HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7aac3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 3\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8be554",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bb2c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq_to_edits(INPUT_DIM,OUTPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,DROPOUT,PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f809680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 155,823 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8b3aeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.6149,  0.9273,  0.5583,  ..., -0.0922, -0.0798, -1.2843],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.4221,  0.6307, -0.1291,  ...,  0.1045,  0.1854, -0.4767],\n",
       "        [ 0.2783,  0.4995,  0.2494,  ..., -0.3274, -0.3004,  0.0930],\n",
       "        [ 0.7090,  0.6810, -1.2128,  ...,  0.6917,  1.4080, -1.0227]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-tained embeddings \n",
    "\n",
    "pretrained_embeddings = tokens.vocab.vectors\n",
    "\n",
    "model.token_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fef5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "\n",
    "model.token_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.token_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcd22ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "n_total_steps = len(train_data_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73910604",
   "metadata": {},
   "source": [
    "### model train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e032cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e234b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231aae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca555de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a3392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf99dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96e9a17c",
   "metadata": {},
   "source": [
    "### Model(rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80f71b21",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_6154/2935548662.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medits_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3089\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3090\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(tokens_list)\n",
    "        loss = criterion(outputs, edits_list)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "921e2892",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Example' and 'Example'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_6154/2892143589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mn_class_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0medits_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             self.batches = batch(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    250\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    251\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn3/lib/python3.7/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m\"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'Example' and 'Example'"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "epoch_loss = 0 \n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    \n",
    "    for batch in test_data_iterator:\n",
    "        tokens_list = batch.tokens.to(device)\n",
    "        edits_list = batch.edits.to(device)\n",
    "        \n",
    "        outputs = model(tokens_list)\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "        loss = criterion(predictions, batch.edits_list)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "print(f\"avg loss {epoch_loss/n_total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1b254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n",
    "\n",
    "for batch in train_data_iterator:\n",
    "    x = emb_1(batch.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a100a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = 40\n",
    "C_in = 40\n",
    "L_in = 100\n",
    "inputs = torch.rand([N, C_in, L_in])\n",
    "padding = 3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "C_out = 10\n",
    "x = torch.nn.Conv1d(C_in, C_out, kernel_size, stride=stride, padding=padding)\n",
    "y = x(inputs)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2200c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 20])\n",
      "tensor([[[-1.3545,  0.6273, -1.1048,  1.5357,  1.0720,  1.7085,  1.8519,\n",
      "          -0.6226, -0.2996, -1.9516, -0.0344,  0.2025, -0.4796,  1.0473,\n",
      "           0.5128,  0.8766,  0.2924,  1.6486, -0.0163,  1.0972]],\n",
      "\n",
      "        [[ 0.4953,  0.9346,  0.1665,  0.2923,  1.4549, -0.3342,  1.7224,\n",
      "           0.6442, -0.6272, -0.2344, -0.0523, -1.8436, -0.5091, -0.3261,\n",
      "           0.2669,  1.6999, -0.7664, -0.2777, -1.1153,  1.2729]]])\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "93df79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 2,stride=2)(x_input)\n",
    "b = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 1,stride=1)(x_input)\n",
    "c = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 3,stride=3)(x_input)\n",
    "d = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 4,stride=4)(x_input)\n",
    "d = nn.Conv1d(in_channels =1 ,out_channels= 1,kernel_size = 5,stride=5)(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b0559c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = torch.Size([2, 1, 10])\n",
      "b = torch.Size([2, 1, 20])\n",
      "c = torch.Size([2, 1, 6])\n",
      "d = torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")\n",
    "print(f\"d = {d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "225d29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = [a.squeeze(1),b.squeeze(1),c.squeeze(1),d.squeeze(1)]\n",
    "e = torch.cat(pooled,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6afeba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = torch.Size([2, 40])\n"
     ]
    }
   ],
   "source": [
    "print(f\"e = {e.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c601614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9ad8b",
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a1 = {a1.shape}\")\n",
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba74a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bababa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19848762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7ee0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn3]",
   "language": "python",
   "name": "conda-env-nn3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "135px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
