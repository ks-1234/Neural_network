{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26723d",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "\n",
    "### Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987a224",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "\n",
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32f2f50",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Transformer\n",
    "from torchmetrics.functional import precision_recall,f1_score,accuracy\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06f1da",
   "metadata": {},
   "source": [
    "''' instal packages '''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d488e8f",
   "metadata": {},
   "source": [
    "conda install pytorch=1.9 torchvision torchaudio -c pytorch\n",
    "conda install -y torchtext==0.10 -c pytorch\n",
    "conda install -c conda-forge torchmetrics\n",
    "pip install seqeval\n",
    "conda install -c conda-forge matplotlib-base\n",
    "conda install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cba2cf",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d545ea5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #1,2,4,8,16,32,64,128,256,512,1028\n",
    "path = \"data_filter/\"\n",
    "# path = \"small_data/\" \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reference \n",
    "# ['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', \n",
    "#  'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', \n",
    "#  'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4c24",
   "metadata": {},
   "source": [
    "#### seed initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "11c135d1",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "torch.backends.cudnn.determininistic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9905bc",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa85690",
   "metadata": {},
   "source": [
    "To do\n",
    "1. fetch the data from the file. Using field and tabulardataset\n",
    "2. Create a iterator to loop over the data. Also separate batchs with similar length and pad the extra space\n",
    "3. Build a vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ef1ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "# removeTokens = lambda values: values[1:-1]   # function to remove [CLS] and [SEP] from the data set\n",
    "\n",
    "tokens = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"<pad>\", init_token = '<sos>', eos_token = '<eos>')\n",
    "edits = Field(sequential=True,use_vocab=True,batch_first = True,lower=True,pad_token=\"<pad>\", init_token = '<sos>', eos_token = '<eos>')\n",
    "\n",
    "fields = {'tokens':('tokens',tokens),'labels':('edits',edits)}\n",
    "\n",
    "train_data, val_data, test_data = TabularDataset.splits(path=path,train='ptrain.jsonl',validation='val.jsonl',\n",
    "                                                        test='test.jsonl',format='json',fields=fields)\n",
    "\n",
    "# train_data is dataset with edits and tokens pair. in edits and tokens list of string is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "70b1ac6d",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# step 2  Build a vocab\n",
    "\n",
    "tokens.build_vocab(train_data,min_freq=1)\n",
    "edits.build_vocab(train_data,min_freq=1)\n",
    "\n",
    "# os =  RandomOverSampler()\n",
    "# X_train_res, y_train_res = os.fit_sample(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b2fb1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (Create a iterator to loop over the data. Also separate batchs with \n",
    "#         similar length and pad the extra space)\n",
    "\n",
    "train_data_iterator = BucketIterator(train_data,train=True,\n",
    "                                            batch_size=BATCH_SIZE, device= device,shuffle=True)#,sort_within_batch=False)\n",
    "\n",
    "val_data_iterator =BucketIterator(val_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n",
    "test_data_iterator =BucketIterator(test_data,BATCH_SIZE,train=False,sort=False, device= device)#,sort_within_batch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdb648",
   "metadata": {},
   "source": [
    "#### data processing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "92295815",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49896"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "10798651",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12374"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e97f75e0",
   "metadata": {
    "hide_input": false,
    "lang": "en"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.TabularDataset object at 0x7f856837b250>\n",
      "dict_keys(['tokens', 'edits'])\n",
      "dict_values([['[cls]', 'alistair', 'darling', 'is', 'expected', 'to', 'announce', 'details', 'of', 'tax', 'cuts', 'and', 'plans', 'to', 'increases', 'public', 'spending', '[sep]'], ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$append_.', '$keep']])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)              # Tabular Data set object\n",
    "\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7074d5c4",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 53])\n",
      "torch.Size([32, 53])\n"
     ]
    }
   ],
   "source": [
    "batch_1 = next(iter(train_data_iterator))\n",
    "print(batch_1.edits.shape)\n",
    "print(batch_1.tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3ef8b425",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.stoi['0'] = 0\n",
      "tokens.vocab.itos[0] = <sos>\n",
      "edits.vocab.stoi['$keep'] = 0\n",
      "edits.vocab.itos[1] = <pad>\n"
     ]
    }
   ],
   "source": [
    "#string to index\n",
    "print(f\"tokens.vocab.stoi['0'] = {tokens.vocab.stoi['']}\")\n",
    "print(f\"tokens.vocab.itos[0] = {tokens.vocab.itos[2]}\")\n",
    "print(f\"edits.vocab.stoi['$keep'] = {edits.vocab.stoi['0']}\")\n",
    "print(f\"edits.vocab.itos[1] = {edits.vocab.itos[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "547693f9",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab) = 64172\n",
      "len(edits.vocab) = 24\n"
     ]
    }
   ],
   "source": [
    "#length of vocabular create from the data set\n",
    "print(f\"len(tokens.vocab) = {len(tokens.vocab)}\")\n",
    "print(f\"len(edits.vocab) = {len(edits.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8d3243d5",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.vocab.freqs.keys()) = 64168\n",
      "len(edits.vocab.freqs.keys()) = 20 \n",
      "\n",
      "edits.vocab.freqs = Counter({'$keep': 1191726, '$delete': 39872, '$replace_.': 7235, '$replace_,': 7183, '$transform_agreement_singular': 6220, '$append_.': 5167, '$append_,': 4905, '$append_the': 4686, '$replace_to': 3634, '$replace_the': 3574, '$replace_of': 3458, '$transform_verb_vbz_vb': 3253, '$replace_in': 2898, '$transform_verb_vbg_vb': 2714, '$transform_verb_vbn_vb': 2637, '$append_to': 2499, '$append_of': 2413, '$transform_agreement_plural': 2340, '$append_and': 2272, '$append_a': 2204})\n"
     ]
    }
   ],
   "source": [
    "# no. of unique words in tokens and edits\n",
    "print(f\"len(tokens.vocab.freqs.keys()) = {len(tokens.vocab.freqs.keys())}\")\n",
    "print(f\"len(edits.vocab.freqs.keys()) = {len(edits.vocab.freqs.keys())} \\n\")\n",
    "print(f\"edits.vocab.freqs = {edits.vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ae29120c",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.vocab.vectors.shape = <torchtext.legacy.data.field.Field object at 0x7f8568393fd0>\n",
      "edits.vocab.vectors.shape = <torchtext.legacy.data.field.Field object at 0x7f8568393410>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shape of vocabular create from the data set\n",
    "print(f\"tokens.vocab.vectors.shape = {tokens}\")\n",
    "print(f\"edits.vocab.vectors.shape = {edits}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7484bbd",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "source": [
    "build vocab just takes unique tokens from the dataset and given a position and stores\n",
    "as a dictionary. when it is applied to the dataset the result comming from the \n",
    "bucket iteartor is just a postion no. from the build vocab and the rest is padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259fcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e1207c94",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_layer:int, # num of layer in encoder\n",
    "                 decoder_layer:int, #num of layer in decoder\n",
    "                 emb_dim:int, #embedding dimension\n",
    "                 head:int, #num of head\n",
    "                 src_vocab_size:int,\n",
    "                 trg_vocab_size:int,\n",
    "                 feedforward_dim:int, \n",
    "                 src_pad_idx:int,\n",
    "                 trg_pad_idx:int,\n",
    "                 device:str,\n",
    "                 dropout:float=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.head = head\n",
    "        self.emb_dim = emb_dim\n",
    "        self.device = device\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.src_embedding_layer = nn.Embedding(src_vocab_size,emb_dim,device=device)\n",
    "        self.trg_embedding_layer = nn.Embedding(trg_vocab_size,emb_dim,device=device)\n",
    "        \n",
    "        #transformer layer\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim,\n",
    "                                       nhead = head,\n",
    "                                       num_encoder_layers = encoder_layer,\n",
    "                                       num_decoder_layers = decoder_layer,\n",
    "                                       dim_feedforward=feedforward_dim,\n",
    "                                       dropout = dropout,\n",
    "                                       batch_first = True,\n",
    "                                       device = device)\n",
    "        \n",
    "        #Linear Layer\n",
    "        self.linear_layer = nn.Linear(emb_dim,trg_vocab_size)\n",
    "        \n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def positional_embedding(self,length = 200):\n",
    "        \n",
    "        position = torch.arange(length).unsqueeze(1).to(self.device)    # [batch_size, num_of_tokens]\n",
    "        denominator = torch.exp(torch.arange(0, self.emb_dim, 2) * (-math.log(10000.0) / self.emb_dim))\n",
    "        \n",
    "        position_embedding = torch.zeros((length, self.emb_dim))\n",
    "        position_embedding[:,0::2] = torch.sin(position*denominator)\n",
    "        position_embedding[:,1::2] = torch.cos(position*denominator)\n",
    "        \n",
    "        position_embedding = position_embedding.unsqueeze(0)\n",
    "        # position_embedding = (1,lenght,emb_dim)\n",
    "        \n",
    "        return position_embedding\n",
    "                                                    \n",
    "        \n",
    "    def make_padding_mask(self,template,idx):\n",
    "        #mask = [batch size, src_len/trg_len]\n",
    "        return (template == idx)\n",
    "    \n",
    "    def trg_mask(self,trg):\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_mask = torch.triu(torch.ones((trg_len,trg_len),device=self.device)).bool()\n",
    "        #trg_mask = (trg_len,trg_len)\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self,\n",
    "                src : Tensor, #(batch_size,src_len)\n",
    "                trg : Tensor): #(batch_size,trg_len) \n",
    "                #in this case src_len == trg_len\n",
    "               \n",
    "        \n",
    "        batch_size , src_len  = src.shape\n",
    "        trg_len = src_len # depends upon the senario in our case source and target lenght are same\n",
    "        \n",
    "        # Applying embedding layer\n",
    "        \n",
    "        src_emb = self.src_embedding_layer(src)+self.positional_embedding(src_len)\n",
    "        trg_emb = self.trg_embedding_layer(trg)+self.positional_embedding(trg_len)\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        trg_emb = self.dropout(trg_emb)\n",
    "        # print(f\"src_emb {src_emb[0]}\")\n",
    "        # print(f\"trg_emb {trg_emb[0]}\")\n",
    "        # src_emb = trg_emb = (batch_size,src_len/trg_len,emb_dim)\n",
    "        \n",
    "        trg_mask = self.trg_mask(trg)\n",
    "        src_pad_mask = self.make_padding_mask(src,self.src_pad_idx)\n",
    "        trg_pad_mask = self.make_padding_mask(trg,self.trg_pad_idx)\n",
    "        \n",
    "        print(f\"batch_size {batch_size}\")\n",
    "        print(f\"src {src.shape}\")\n",
    "        print(f\"trg {trg.shape}\")\n",
    "        print(f\"trg_mask {trg_mask} type {trg_mask.dtype}\")\n",
    "        print(f\"src_pad {src_pad_mask[0]} type {src_pad_mask.dtype}\")\n",
    "        print(f\"trg_pad {trg_pad_mask[0]} type {trg_pad_mask.dtype}\")\n",
    "        print(f\"src {src[0]}\")\n",
    "        print(f\"trg {trg[0]}\")\n",
    "            \n",
    "        # Apply transformer layer\n",
    "        transformer_output = self.transformer(src_emb, #(batch_size,src_len,emb_dim)\n",
    "                                   trg_emb, #(batch_size,trg_len,emb_dim) \n",
    "                                   tgt_mask=trg_mask, #(trg_len,trg_len)\n",
    "                                   src_key_padding_mask=src_pad_mask, #[batch size, src_len]\n",
    "                                   tgt_key_padding_mask=trg_pad_mask) #[batch size, trg_len]\n",
    "        \n",
    "        # transformer_output = (batch_size,trg_len,emb_dim)\n",
    "        print(f\"transformer_output {transformer_output.shape} \")\n",
    "        print(f\"transformer_output {transformer_output[0]}\")\n",
    "        # Apply Linear layer\n",
    "        output = self.linear_layer(transformer_output)\n",
    "        print(f\"output {output[0]}\")\n",
    "        print(f\"output {output.shape} \")\n",
    "        # output = (batch_size,trg_len,num_class)\n",
    "        \n",
    "        return output.permute(0,2,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0798e",
   "metadata": {},
   "source": [
    "\n",
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "80f71b21",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,data_iterator,optimizer,criterion,clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss,acc,f1_point = 0,0,0\n",
    "        \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        \n",
    "        # Make the gradient vector to zero  \n",
    "        # So not to add previous gradient values with the new gradient value\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # converting to cpu or gpu variable\n",
    "        src = batch.tokens.to(device)\n",
    "        trg = batch.edits.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        # get output from the model\n",
    "        output = model(src, trg)\n",
    "        # outputs = [Batch_size,num_class,trg_len] \n",
    "        # print(f\"out dim = {output.shape}\")\n",
    "        # print(f\"trg dim = {trg.shape} \")\n",
    "        \n",
    "        loss = criterion(output, trg[:,1:])\n",
    "\n",
    "        # Backward and optimize\n",
    "        \n",
    "        # to calculate gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # to make the updates in the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(loss.item())\n",
    "        \n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        # print(f\"predicted shape = {predicted.shape}\")\n",
    "        # print(f\"trg shape = {trg.shape}\")\n",
    "        print(predicted)\n",
    "        acc += accuracy(predicted, trg).item()       # TP+TN / TP+TN+FP+FN\n",
    "        f1_point += f1_score(predicted, trg,average=\"macro\",num_classes=22,mdmc_average='global').item()\n",
    "        # f1_score = 2(precission*recall)/(precission+recall)\n",
    "        \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    f1_point = f1_point / len(data_iterator)\n",
    "    epoch_loss = epoch_loss / len(data_iterator)\n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss,acc,f1_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "921e2892",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss,acc,f1_point = 0,0,0\n",
    "    \n",
    "    testing = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            \n",
    "            # converting to cpu or gpu variable\n",
    "            src = batch.tokens.to(device)\n",
    "            trg = batch.edits.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            # get output from the model\n",
    "            output = model(src, trg)\n",
    "            # outputs = [Batch_size,num_class,trg_len] \n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            \n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "\n",
    "            acc += accuracy(predicted, trg).item()       # TP+TN / TP+TN+FP+FN\n",
    "            f1_point += f1_score(predicted, trg,average=\"macro\",num_classes=22,mdmc_average='global').item()\n",
    "            # f1_score = 2(precission*recall)/(precission+recall)\n",
    "            \n",
    "    acc = 100.0 * acc / len(data_iterator)\n",
    "    f1_point = f1_point / len(data_iterator)\n",
    "    epoch_loss = epoch_loss / len(data_iterator)\n",
    "    \n",
    "    #to return the avg loss for this epoch to train the model\n",
    "    return (epoch_loss,acc,f1_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a1f239a6",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def baruni_metric(dataloader):\n",
    "    model.eval()\n",
    "    crct_class = 0\n",
    "    incrct_class = 0\n",
    "    other_class = 0\n",
    "    for dat in dataloader:\n",
    "        \n",
    "        X = dat.tokens\n",
    "        Y = dat.edits\n",
    "    #     print(\"here: \",len(X))\n",
    "        \n",
    "        for token,edit in zip(X,Y):\n",
    "        #move to GPU\n",
    "            x,y = token.to(device), edit.to(device)\n",
    "            # Compute prediction error\n",
    "            x = x.unsqueeze(0)\n",
    "            y = y.unsqueeze(0)\n",
    "            # x = [1,num_token]\n",
    "    #         print(x.size(),\": x\")\n",
    "    #         print(\"y: \", y.size())\n",
    "\n",
    "            check_pred = model(x,y)\n",
    "    #         print(\"check_pred\",check_pred.size())\n",
    "            check_pred = torch.argmax(check_pred, dim = 1)\n",
    "        #     print(check_pred)\n",
    "\n",
    "        #     print(y.size())\n",
    "        #     break\n",
    "\n",
    "            for i in range(len(y[0])):\n",
    "                if y[0][i] not in [1,0,2]:\n",
    "                    other_class += 1\n",
    "                    if y[0][i] == check_pred[0][i] :\n",
    "                        crct_class += 1\n",
    "                    else:\n",
    "                        incrct_class += 1\n",
    "    \n",
    "    return other_class,crct_class,incrct_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad412d2",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b07bcb53",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "num_epochs = 50\n",
    "clip = 0.1\n",
    "num_encoder_layer = 3\n",
    "num_decoder_layer = 3\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "HIDDEN_DIM = 128\n",
    "EMBEDDING_DIM = 100  #vocabular size, dim\n",
    "layer = 1\n",
    "heads = 2\n",
    "weight_decay = 0\n",
    "amsgrad = False\n",
    "DROPOUT = 0.1\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]\n",
    "EDIT_PAD_IDX = edits.vocab.stoi[edits.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074921ca",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c38ca13d",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# model is created\n",
    "\n",
    "model = Seq2SeqTransformer(num_encoder_layer,num_decoder_layer,EMBEDDING_DIM,heads,INPUT_DIM,OUTPUT_DIM,HIDDEN_DIM,PAD_IDX,EDIT_PAD_IDX,device,DROPOUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "40d081c6",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=EDIT_PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#,weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "\n",
    "n_total_steps = len(train_data_iterator)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d4689f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the pre-tained embeddings \n",
    "\n",
    "# pretrained_embeddings = tokens.vocab.vectors\n",
    "# # model = torch.load(path+'/model.pt')\n",
    "# model.src_embedding_layer.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "af276631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize zero weights for unknown and padding tokens.\n",
    "\n",
    "model.src_embedding_layer.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.src_embedding_layer.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9fa205d5",
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,943,992 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters are printed\n",
    "\n",
    "count_parameters= lambda model:sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d01afdc1",
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32\n",
      "src torch.Size([32, 42])\n",
      "trg torch.Size([32, 42])\n",
      "trg_mask tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [False,  True,  True,  ...,  True,  True,  True],\n",
      "        [False, False,  True,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False,  True,  True],\n",
      "        [False, False, False,  ..., False, False,  True]]) type torch.bool\n",
      "src_pad tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True]) type torch.bool\n",
      "trg_pad tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True]) type torch.bool\n",
      "src tensor([    2,     6,     4,  1214,    16,  9537,  5585, 10620,    12,  4749,\n",
      "         2298,    40,   648,     9,   176,     8,     7,     3,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "trg tensor([ 2,  4,  4,  4,  4,  4, 10,  4,  4, 18,  4,  4,  4,  4,  4,  4,  4,  3,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1])\n",
      "transformer_output torch.Size([32, 42, 100]) \n",
      "transformer_output tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SelectBackward>)\n",
      "output tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SelectBackward>)\n",
      "output torch.Size([32, 42, 24]) \n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_data_iterator))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "tokens_list = batch.tokens.to(device)\n",
    "edits_list = batch.edits.to(device)\n",
    "\n",
    "# Forward pass\n",
    "# print(f\"tokens_list {tokens_list[0]}\")\n",
    "# get output from the model\n",
    "outputs = model(tokens_list, edits_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6de421d0",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e5f1",
   "metadata": {},
   "source": [
    "### Uploading model to tensor board for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "063a7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f'runs/transformer_test_1.1_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabddc3b",
   "metadata": {},
   "source": [
    "#### Train and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82b0f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,train_acc,train_f1_score,val_loss,val_acc,val_f1_score = 0,0,0,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2223181f",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "nan\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "time take is 0.010 min\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The highest label in `target` should be smaller than `num_classes`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_14276/2826425227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_f1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_f1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s7/9q6vwr352txcx0rs9vflhmrj4x6wvg/T/ipykernel_14276/2439607420.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# TP+TN / TP+TN+FP+FN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mf1_point\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmdmc_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# f1_score = 2(precission*recall)/(precission+recall)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torchmetrics/functional/classification/f_beta.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(preds, target, beta, average, mdmc_average, ignore_index, num_classes, threshold, top_k, multiclass)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[1;32m    352\u001b[0m     return fbeta_score(\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdmc_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torchmetrics/functional/classification/f_beta.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(preds, target, beta, average, mdmc_average, ignore_index, num_classes, threshold, top_k, multiclass)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torchmetrics/functional/classification/stat_scores.py\u001b[0m in \u001b[0;36m_stat_scores_update\u001b[0;34m(preds, target, reduce, mdmc_reduce, num_classes, top_k, threshold, multiclass, ignore_index, mode)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_input_format_classification\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k, ignore_index)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0m_check_num_classes_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTICLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTIDIM_MULTICLASS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0m_check_num_classes_mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILABEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0m_check_num_classes_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn1/lib/python3.7/site-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_check_num_classes_mc\u001b[0;34m(preds, target, num_classes, multiclass, implied_classes)\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The highest label in `target` should be smaller than `num_classes`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mimplied_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The size of C dimension of `preds` does not match `num_classes`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The highest label in `target` should be smaller than `num_classes`."
     ]
    }
   ],
   "source": [
    "start = time.perf_counter_ns()\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss,train_acc,train_f1_score = train_model(model,train_data_iterator,optimizer,criterion,clip)\n",
    "        val_loss,val_acc,val_f1_score = evaluate_model(model, val_data_iterator, criterion)\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            pass\n",
    "        print (f'''Epoch [{epoch+1}/{num_epochs}],\n",
    "        Train:       Loss: {train_loss:.3f}, Accuracy: {train_acc:.3f},  F1 score: {train_f1_score:.3f}\n",
    "        Validation:  Loss: {val_loss:.3f}, Accuracy: {val_acc:.3f},  F1 score: {val_f1_score:.3f}''')\n",
    "            \n",
    "        \n",
    "        # including the loss, accuracy and f1 score to tensor board\n",
    "        writer.add_scalars('LOSS',     { 'Train' : train_loss   ,'Validation' : val_loss    },  epoch)\n",
    "        writer.add_scalars('ACCURACY', { 'Train': train_acc     ,'Validation': val_acc      }, epoch)\n",
    "        writer.add_scalars('F1 SCORE', { 'Train': train_f1_score,'Validation': val_f1_score }, epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "finally:\n",
    "    end = time.perf_counter_ns()\n",
    "    timetaken = (end-start)*1.66667*10**-11\n",
    "    print(f\"time take is {timetaken:.3f} min\")\n",
    "#     torch.save(model, path+'/model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_accuracy,f1 = evaluate_model(model, test_data_iterator, criterion)\n",
    "print(f\"Test Loss {test_loss:.3f} Test accuracy {test_accuracy:.3f} F1 score {f1:.3f}\")\n",
    "total,crt,incrt = baruni_metric(train_data_iterator)\n",
    "print(f\"total no.of class excet<keep> {total}\\ncrt pred {crt}\\nincrt pred{incrt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the hyerparameter to the tensor board\n",
    "writer.add_hparams({'learning_rate' : learning_rate, \n",
    "                    'Num_epochs': num_epochs,\n",
    "                    'layer': layer,\n",
    "                    'input_dim' : INPUT_DIM,\n",
    "                    'output_dim' : OUTPUT_DIM,\n",
    "                    'hidden_dim' : HIDDEN_DIM,\n",
    "                    'embedding_dim' : EMBEDDING_DIM,\n",
    "                    'droppout' : DROPOUT,\n",
    "                    'train_data_len': len(train_data),\n",
    "                    'Val_data_len': len(test_data),\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'weight_decay' : weight_decay,\n",
    "                    'amsgrad' : amsgrad\n",
    "                  },\n",
    "                  {\n",
    "                     \"total_num_of_class_except_keep\":total,\n",
    "                      \"crt\":crt,\"incrt\":incrt,\n",
    "                      \"test_loss\":test_loss,\n",
    "                      \"test_accuracy\":test_accuracy,\n",
    "                      \"test_f1_score\":f1\n",
    "                  })\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a041d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cb98584e",
   "metadata": {},
   "source": [
    "#### 10-08-2022\n",
    "path = data\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "clip = 1\n",
    "INPUT_DIM = len(tokens.vocab)\n",
    "OUTPUT_DIM = len(edits.vocab)\n",
    "EMBEDDING_DIM = tokens.vocab.vectors.shape[1]   #vocabular size, dim\n",
    "CHANNELS = [1,3,5,7,9,11,13,15]\n",
    "layer = len(CHANNELS)+3\n",
    "KERNAL_SIZE = 5\n",
    "# weight_decay = 0.001\n",
    "# amsgrad = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = tokens.vocab.stoi[tokens.pad_token]\n",
    "UNK_IDX = tokens.vocab.stoi[tokens.unk_token]\n",
    "EDIT_PAD_IDX = edits.vocab.stoi[edits.pad_token]\n",
    "\n",
    "crct_class 140\n",
    "incrct_class 139\n",
    "other_class 279\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9b4e3",
   "metadata": {},
   "source": [
    "### Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['plus', ',', 'the', 'novelty', 'of', 'the', 'iphone', 'won', \"'t\", 'wear', 'off', ',', 'as', 'it', 'may', 'with', 'a', 'camcorder', ';', 'and', 'over', 'these', 'video', 'apps', 'have', 'fun', 'effects', 'that', 'a', 'camcorder', 'can', \"'t\", 'match', '.']\n",
    "l1=['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$replace_might', '$keep', '$keep', '$keep', '$replace_,', '$keep', '$delete', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep']\n",
    "print(len(l))\n",
    "print(len(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d700cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[25, 13, 14, 29, 13, 13, 13, 25, 13, 13, 25, 21, 13, 13, 29, 13,  2,  2,\n",
    "        13, 13, 13, 25]\n",
    "b=[2, 2, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e32a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70a859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185d75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings, dim = 10,4 #10 - # of vocac size 4 - # of emdebbing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b779edf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 2, 3)\n",
    "input = torch.randn(2, 1, 5) # (batch size, no. of channel, # of words)\n",
    "# x = emb_1(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dce7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4c31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ce96f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding as emb\n",
    "\n",
    "emb_1 = emb(188, 50,requires_grad = True)\n",
    "print(f\"embedding {emb_1}\")\n",
    "print(f\"embedding weight's shape {emb_1.weight.shape}\") #requires_grad=True therefore the matrix is learnable\n",
    "\n",
    "print(f\"values of weight {emb_1.weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8570b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,4],[1, 2, 3]])\n",
    "print(x.shape)\n",
    "x=x.repeat(4, 2)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e442ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 1)(x_input)\n",
    "b = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 2)(a)\n",
    "c = nn.Conv2d(in_channels =1 ,out_channels= 1,kernel_size = 6)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.shape}\")\n",
    "print(f\"b = {b.shape}\")\n",
    "print(f\"c = {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.squeeze(1).reshape(2,5,-1)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f = {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = nn.Conv1d(in_channels =1 ,out_channels= 4,kernel_size = 3)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"g = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcc818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.permute(0,2,1).reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f9673",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48fbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9f9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df482c43",
   "metadata": {},
   "source": [
    "#### ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act = torch.Tensor([[2,1],[1,3]]).int()\n",
    "# batch size, tokens\n",
    "print(f\" output shape {y_act.shape}\")\n",
    "s1 = torch.Tensor([[[0,2,3,0],[5,0,1,3]],[[1,5,2,3],[1,5,6,13]]])\n",
    "# batch size, tokens, emb lay\n",
    "\n",
    "print(f\" predicted shape {s1.shape}\")\n",
    "y_pred = torch.argmax(s1,dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f1_score(y_act,s1))\n",
    "print(accuracy_score(y_act,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956cd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(y_pred, y_act, average=\"macro\",num_classes=4,mdmc_average='samplewise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7012aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_pred,y_act,average=\"macro\",num_classes=4,mdmc_average='global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "print(f1_score(y_true, y_pred))\n",
    "\n",
    "y_act = [[2,1],[1,3]]\n",
    "s1 = [[[0,2,3,0],[5,0,1,3]],[[1,5,2,3],[1,5,6,13]]])\n",
    "# batch size, tokens, emb lay\n",
    "\n",
    "print(f\" predicted shape {s1.shape}\")\n",
    "y_pred = torch.argmax(s1,dim=-1)\n",
    "print(f1_score(y_act,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae689e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199b663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0228ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r = precision_recall(y_pred, y_act, average=\"macro\",num_classes=4,mdmc_average='global')\n",
    "print(p)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2*p.item()*r.item())/(p.item()+r.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa043f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f20f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1154a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf58bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928294b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68931167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(y_pred,y_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa505f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e81a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5+0.5+0.5+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b626a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f25d69",
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b1 = nn.Conv1d(in_channels =1 ,out_channels= 10,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =10 ,out_channels= 10,kernel_size = 3,padding=\"same\")(b1)\n",
    "d1 = nn.Conv1d(in_channels =10 ,out_channels= 10,kernel_size = 5,padding=\"same\")(c1)\n",
    "d2 = nn.Conv1d(in_channels =10 ,out_channels= 1,kernel_size = 1,padding=\"same\")(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"b1 = {b1.shape}\")\n",
    "print(f\"c1 = {c1.shape}\")\n",
    "print(f\"d1 = {d1.shape}\")\n",
    "print(f\"d2 = {d2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = d2.reshape(2,4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape \n",
    "# 1 -> ed*num filter\n",
    "# 2*filtersize -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "15*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 2,padding=\"same\")(x_input)\n",
    "b1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 1,padding=\"same\")(x_input)\n",
    "c1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 3,padding=\"same\")(x_input)\n",
    "d1 = nn.Conv1d(in_channels =4 ,out_channels= 4,kernel_size = 4,padding=\"same\")(x_input)\n",
    "e1 = torch.cat([a1,b1,c1,d1],dim =1)\n",
    "f1 = nn.Conv1d(in_channels = e1.shape[1],out_channels= 25,kernel_size = 1)(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5da08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled1 = [a1,b1,c1,d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"e1 = {e1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9cd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.arange(0,10,1)\n",
    "y1 = np.random.randn(10)\n",
    "y3 = np.random.randn(10)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, sharex='col')\n",
    "ax[0].plot(x,y1)\n",
    "ax[1].plot(x,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8b8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9279fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3bea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde2da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35753d30",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.sqrt(torch.FloatTensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "\n",
    "# print(x_input)\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conv 1D layers \n",
    "conv_layers = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                            out_channels= 1,\n",
    "                                            kernel_size = filter_size,\n",
    "                                            padding = \"same\") \n",
    "                                  for filter_size in [1,3,5]])\n",
    "\n",
    "final_layer = nn.Linear(5*3, 3)\n",
    "\n",
    "# Dropout layers\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokens size is [batch_size, max sentence size in the batch/token size]\n",
    "# token_embeddings = [2, 1, 4*5]\n",
    "conved = x_input\n",
    "for conv_layer in conv_layers:\n",
    "    conved = F.relu(conv_layer(conved))\n",
    "print(len(conved))\n",
    "concat = dropout(torch.cat(conved, dim = 1))\n",
    "# [batch size, embedding dimension*num_of_tokens*num_of_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conved = concat.reshape(2,4,-1)\n",
    "# conved = [batch_size, num_of_tokens, embedding_dimension*num_of_filters]\n",
    "conved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = final_layer(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a914c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd4dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e8b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"tokens\": [\"Alistair\", \"Darling\", \"is\", \"expected\", \"to\", \"announce\", \"details\", \"of\", \n",
    "                \"tax\", \"cuts\", \"and\", \"plans\", \"to\", \"increases\", \"public\", \"spending\"], \n",
    "     \"labels\": [\"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \n",
    "                \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$TRANSFORM_VERB_VBZ_VB\", \"$KEEP\", \"$APPEND_.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ed987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_accuracy(test_data,tokens,edits,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$KEEP\", \"$TRANSFORM_VERB_VBZ_VB\", \"$KEEP\", \"$APPEND_.\"] \n",
    "b = ['$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$keep', '$transform_verb_vbz_vb', '$keep', '$keep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(a,b):\n",
    "    print(i.lower()==j,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17b890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371f769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = torch.Tensor([[[1,2,10],[3,4,11]],[[5,6,12],[7,8,13]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5e895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = [0] * len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658daf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = torch.randn([2,4,5])\n",
    "filterss = [2,3,4]\n",
    "inf = 1\n",
    "x_input = x_input.reshape(2,-1).unsqueeze(1)\n",
    "print(x_input.shape)\n",
    "# print(x_input)\n",
    "\n",
    "conv_layers = nn.ModuleList()\n",
    "\n",
    "for i,filter_size in enumerate(filterss):\n",
    "    conv_layers.append(nn.Conv1d(in_channels = inf,out_channels= 10,\n",
    "                                 kernel_size = filter_size,padding = \"same\"))\n",
    "    inf = 10\n",
    "\n",
    "con = x_input\n",
    "print(\"in\",con.shape)\n",
    "\n",
    "for i, conv_layer in enumerate(conv_layers):\n",
    "    #pass through convolutional layer\n",
    "    print(f\"{i} input {con.shape}\")\n",
    "    \n",
    "    conved = F.relu(conv_layer(con))\n",
    "    print(f\"{i} iiiii {conved.shape}\")\n",
    "    conved = conved + con\n",
    "    con = conved\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e81c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732268a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deb74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144754b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3415ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e5dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad7ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce485b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afe7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46f399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd35a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40550a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ced2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data,label) in enumerate(train_data_iterator):\n",
    "    print(label)\n",
    "    break\n",
    "class_weights= torch.Tensor([1/value for key,value in edits.vocab.freqs.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [batch.edits for batch in train_data_iterator]\n",
    "l1 = [lst for edit in l1 for lst in edit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5393e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fc251",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "for data in train_data_iterator:\n",
    "        if len(files) > 0:\n",
    "            class_weights.append(1/len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (token, edit) in enumerate(train_data):\n",
    "    class_weight = sum([class_weight for class_weight in eidts])\n",
    "    sample_weights[idx] = class_weight\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d972781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e791ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde83cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549513e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc6dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c91dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assume we have some tensor x with size (b, t, k)\n",
    "x = torch.zeros((64,48,100))\n",
    "\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eb9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd164b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c186c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg = torch.rand((4,5))\n",
    "trg_pad_idx=1\n",
    "trg_len = 5\n",
    "trg_pad_mask=(trg!=trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "print(trg_pad_mask.shape)\n",
    "trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len))).bool()\n",
    "print(trg_sub_mask.shape)\n",
    "trg_mask=trg_pad_mask & trg_sub_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((5,1,1,5))\n",
    "a = (a!=0)\n",
    "print(a.shape)\n",
    "b = torch.tril(torch.ones((5, 5))).bool()\n",
    "print(b.shape)\n",
    "s = a&b\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s[0])\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb22d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.Tensor([[None,1,1,2,3,4,4],\n",
    "     [3,2,3,2,3,2,2]])\n",
    "\n",
    "ll = torch.Tensor([[1,4,1,2,3,4,4],\n",
    "     [3,2,3,2,3,2,2]])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d94c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (l == 1)\n",
    "b = (l == 2)\n",
    "print(a,\"\\n\",b)\n",
    "print(a&b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (l == 1)\n",
    "b = (l == 2)\n",
    "print(a,\"\\n\",b)\n",
    "print(!a & !b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "list1 = [[12,24,36], [3,5,12,24], [36,41,69]]\n",
    "torch.bincount(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99223aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8aeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,0,0],[1,1,0],[1,1,1]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc647b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16433d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (torch.triu(torch.ones((5, 5))) == 1).transpose(0, 1)\n",
    "print(mask)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68152d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(- torch.arange(0, 5, 2)* math.log(10000) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c5ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f019b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aeae6e2",
   "metadata": {},
   "source": [
    "###  THE END"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nn1]",
   "language": "python",
   "name": "conda-env-nn1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "316px",
    "width": "201px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "neural network",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "685px",
    "left": "26px",
    "top": "111.141px",
    "width": "218.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.852,
   "position": {
    "height": "359.852px",
    "left": "725px",
    "right": "20px",
    "top": "109px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1d36ce9c6112765ad20cbf471b33373de7b9da6b2d9a2b18d6540897511eb08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
